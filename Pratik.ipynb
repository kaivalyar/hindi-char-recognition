{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io, morphology, img_as_bool, segmentation\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.ndimage.morphology import binary_fill_holes\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_resize(img):\n",
    "    top = int((224 - img.shape[0])/2)\n",
    "    left = int((224 - img.shape[1])/2)\n",
    "    bottom = 224 - img.shape[0] - top\n",
    "    right = 224 - img.shape[1] - left\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=255)\n",
    "    img = img/255.\n",
    "    img = cv2.resize(img, (64,64)) #KADD\n",
    "    return img\n",
    "\n",
    "def skeletonize(img):\n",
    "    size = np.size(img)\n",
    "    skel = np.zeros(img.shape,np.uint8)\n",
    "    img = cv2.bitwise_not(img)\n",
    "#     element = cv2.getStructuringElement(cv2.MORPH_CROSS,(1,1))\n",
    "#     done = 0\n",
    "#     while( done < 1 ):\n",
    "#         eroded = cv2.erode(img,element)\n",
    "#         temp = cv2.dilate(eroded,element)\n",
    "#         temp = cv2.subtract(img,temp)\n",
    "#         skel = cv2.bitwise_or(skel,temp)\n",
    "#         img = eroded.copy()\n",
    "\n",
    "#         zeros = size - cv2.countNonZero(img)\n",
    "#         if zeros==size:#cv2.countNonZero(img) * 1 >= 0:#\n",
    "#             done += 1\n",
    "#     img = skel\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    erosion = cv2.erode(img,kernel,iterations = 2)\n",
    "    img = cv2.bitwise_not(erosion)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Done\n",
      "200 Done\n",
      "300 Done\n",
      "400 Done\n",
      "500 Done\n",
      "600 Done\n",
      "700 Done\n",
      "800 Done\n",
      "900 Done\n",
      "1000 Done\n",
      "1100 Done\n",
      "1200 Done\n",
      "1300 Done\n",
      "1400 Done\n",
      "1500 Done\n",
      "1600 Done\n",
      "1700 Done\n",
      "1800 Done\n"
     ]
    }
   ],
   "source": [
    "PATH = '../train_images_modified'\n",
    "images = []\n",
    "base_class = []\n",
    "matra_class = []\n",
    "dot_class = []\n",
    "total_class = []\n",
    "for filename in os.listdir(PATH):\n",
    "    if filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(PATH,filename),0)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        a,img = cv2.threshold(img,127,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)#KEDIT\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#         plt.show()\n",
    "# #         kernel = np.ones((5,5),np.uint8)\n",
    "#         img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#         plt.show()\n",
    "        #img = skeletonize(img)\n",
    "        img = pad_resize(img)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        char_arr = filename[:-4].split('_')[3:]\n",
    "        if(len(char_arr)>0):\n",
    "            images.append(img)\n",
    "            char_arr = [int(i) for i in char_arr]\n",
    "            total_class.append(char_arr)\n",
    "            base = [i for i in char_arr if (i>=2308 and i<=2361) or (i==2384) or (i>=2392 and i<=2401) or (i>=2404 and i!=2416 and i!=2417)]  \n",
    "            base_class.append(base[0])\n",
    "            dot = [i for i in char_arr if i in [2416,2417]]\n",
    "            if(len(dot)>0):\n",
    "                dot_class.append(dot[0])\n",
    "            else:\n",
    "                dot_class.append(0)\n",
    "            matra = [i for i in char_arr if (i>=2304 and i<=2307) or (i>=2362 and i<=2380) or (i>=2382 and i<=2383) or (i>=2385 and i<=2391) or (i>=2402 and i<=2403)]\n",
    "            if(len(matra)>0):\n",
    "                matra_class.append(matra[0])\n",
    "            else:\n",
    "                matra_class.append(0)\n",
    "#             if(len(char_arr)>1):          \n",
    "#                 matra_class.append(char_arr[1:])\n",
    "            if len(images)%100==0:\n",
    "                print(\"{} Done\".format(len(images)))\n",
    "            \n",
    "# print(len(images))\n",
    "# print(len(char_class))\n",
    "# print(images[0])\n",
    "# print(char_class[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD9dJREFUeJzt3V+MVOd9xvHvUzBNmkQshClCxu4SGcXioizRyLFlK2C7\njmhqBV9EVpyqggqJG7daaKrEbqXKkVopvgnmonWFajdcuMFOHBfLipJQCq4qVdjjAg5/4pi4WAZh\n77oFkvYiDc6vF3NYnZ3uzJ7dOefMLO/zkdCcc+bP+dmzz5z3/HtfRQRmlpZfG3QBZlY/B98sQQ6+\nWYIcfLMEOfhmCXLwzRLk4JslqK/gS9os6Q1JZyU9UlZRZlYtzfcCHkmLgJ8A9wHngVeBhyLidHnl\nmVkVFvfx3tuAsxHxFoCk/cAWoGvwV6xYEaOjo32s0sx6OXfuHO+//75me10/wb8ReCc3fx74dK83\njI6O0mq1+lilmfXSbDYLva7yg3uSdkhqSWpNTk5WvTozK6Cf4F8AbsrNr86WTRMReyOiGRHNRqPR\nx+rMrCz9BP9VYK2kNZKWAF8EXiynLDOr0rz38SPiqqQ/An4ALAKejohTpVVmZpXp5+AeEfE94Hsl\n1WJmNfGVe2YJcvDNEuTgmyXIwTdLkINvliAH3yxBDr5Zghx8swQ5+GYJcvDNEuTgmyXIwTdLkINv\nliAH3yxBDr5Zghx8swQ5+GYJcvDNEuTgmyXIwTdLkINvliAH3yxBDr5Zghx8swQ5+GYJmjX4kp6W\nNCHpZG7ZckkHJb2ZPS6rtkwzK1ORLf43gc0dyx4BDkXEWuBQNm9mC8SswY+IfwH+q2PxFmBfNr0P\neKDkusysQvPdx18ZERez6XeBlSXVY2Y16PvgXkQEEN2el7RDUktSa3Jyst/VmVkJ5hv89yStAsge\nJ7q9MCL2RkQzIpqNRmOeqzOzMs03+C8CW7PprcCBcsoxszoUOZ33LeDfgE9KOi9pO/B14D5JbwK/\nk82b2QKxeLYXRMRDXZ66t+RazKwmvnLPLEEOvlmCHHyzBDn4Zgly8M0S5OCbJcjBN0uQg2+WIAff\nLEEOvlmCHHyzBDn4Zgly8M0S5OCbJcjBN0uQg2+WIAffLEGz9sBjg3fixIlp8+vXr+/62l27dk1N\n7969u7KabGHzFt8sQQ6+WYLc1B+gK1euTJsfGRkp9fOfeOKJUj+vlyNHjkybP378+NT0tm3bpqaX\nLl1aU0XWi7f4Zgly8M0S5OCbJcj7+AOU3w9e6DZt2tT1uZ07d3Z93eHDhyuqyHopMoTWTZIOSzot\n6ZSk8Wz5ckkHJb2ZPS6rvlwzK0ORpv5V4MsRsQ64HXhY0jrgEeBQRKwFDmXzZrYAFBk77yJwMZv+\nuaQzwI3AFmBT9rJ9wBHgq5VUaUMpfwqvV1O/23sAJM358yOi0Lqsuzkd3JM0CmwAjgIrsx8FgHeB\nlaVWZmaVKRx8SR8Fngd2RsTP8s9F+yd4xp9hSTsktSS1Jicn+yrWzMpRKPiSbqAd+mci4rvZ4vck\nrcqeXwVMzPTeiNgbEc2IaDYajTJqNrM+zbqPr/ZO2FPAmYj4Ru6pF4GtwNezxwOVVGhDq9d+d699\n9/l8vpWryHn8O4E/AH4k6dqJ5z+jHfjnJG0H3gYerKZEMytbkaP6/wp0+/m+t9xyzKwOvnIvEb1O\ngd19993T5sfGxqam3ZnH9cnX6pslyME3S5Cb+texzqvkuhnWG2XyHYnkdz+sf97imyXIwTdLkINv\nliDv4w+p/D7tsWPHur5u2bLp3SDk3/fYY49NTQ/rfnwv4+Pjgy7huuUtvlmCHHyzBLmpP0AbN26c\nNn/58uWp6aL9z1+6dKnUmiwN3uKbJcjBN0uQg2+WIO/jD5GFPK7chg0b+v4Md6JZH2/xzRLk4Jsl\nyE39ROWv+BsdHe36uvwwX513yOWvKLyehgNLgbf4Zgly8M0S5Kb+AnTixImp6c4m9rZt2+b8eUWb\n6Z2vK6MLbRsMb/HNEuTgmyXIwTdLkPfxF4DOfu+LdqJp1s2sW3xJH5L0iqQTkk5J+lq2fI2ko5LO\nSnpW0pLqyzWzMhRp6v8CuCci1gNjwGZJtwOPA7sj4hbgErC9ujLNrExFxs4L4L+z2RuyfwHcA3wp\nW74PeAx4svwS03TlypWp6UE27fMj1uY7CgEYGRmZmi6jxs7Tg75ppzqFDu5JWpSNlDsBHAR+ClyO\niKvZS84DN1ZTopmVrVDwI+KDiBgDVgO3AbcWXYGkHZJaklqTk5PzLNPMyjSn03kRcRk4DNwBjEi6\ntquwGrjQ5T17I6IZEc1Go9FXsWZWjln38SU1gF9GxGVJHwbuo31g7zDwBWA/sBU4UGWhKdi1a9fU\ndH7cuKp1Xoq7fv36OX9GZ0ccZdytl//MXmML2NwVOY+/CtgnaRHtFsJzEfGSpNPAfkl/CRwDnqqw\nTjMrUZGj+q8D/69fpYh4i/b+vpktMKrzlEmz2YxWq1Xb+haaQd3tVsXfQP5qwzJO9fnUXjHNZpNW\nqzXrH5Kv1TdLkINvliDfpGOVyI/Omz867775hoO3+GYJcvDNEuTgmyXI+/hDJH8nXJ135FV9V1z+\nqjt30DkcvMU3S5CDb5YgN/WHSP4UWL4jjnyHF3V4+eWXp6Y3btxY67q7yV8JmP//ZPPjLb5Zghx8\nswQ5+GYJ8j7+kFq6dOnU9M6dO6c9V2cnHWXwKbzh4y2+WYIcfLMEuam/AOzevbvrfBV93ZUhP5R3\nGRba7s2w8xbfLEEOvlmC3NQfoPwVcjC9mT4+Pl7oM3p1O905ym5er5uAyrhab2xsrO/PyJtPl9/W\nnbf4Zgly8M0S5OCbJcj7+EMkf4Ve0X38XnwXm3VTeIufDZV9TNJL2fwaSUclnZX0rKQl1ZVpZmWa\nS1N/HDiTm38c2B0RtwCXgO1lFmZm1SnU1Je0Gvg94K+AP1H7rot7gC9lL9kHPAY8WUGNScrf2NJ5\n1VoZuwFl2LNnz9R0541EZRiWqxCvR0W3+E8AXwF+lc1/HLgcEVez+fPAjSXXZmYVmTX4ku4HJiLi\ntfmsQNIOSS1JrcnJyfl8hJmVrMgW/07g85LOAftpN/H3ACOSru0qrAYuzPTmiNgbEc2IaDYajRJK\nNrN+zWmYbEmbgD+NiPslfRt4PiL2S/pb4PWI+Jte7/cw2dN1XrKb71e/l2EZMrrqDjaG5b9zIalj\nmOyv0j7Qd5b2Pv9TfXyWmdVoThfwRMQR4Eg2/RZwW/klmVnVfOXeAHXeBZc/Jdar44l8E7vq5nCd\nQ1xXcUrQZuZr9c0S5OCbJWhOR/X75aP6xc33iHn+zED+Jp3OTjnqHI23l3wdwzJc10JWx1F9M1ug\nHHyzBDn4Zgny6bwhlT/2smvXrmnP9TrVl99nHsahqzpP2Xm/fjC8xTdLkINvliCfzlvg5nujT9Xy\n/erndz/yowBb+Xw6z8y6cvDNEuTgmyXIp/MWuM7TYd2O2ZRxyW7neHi9xu2z4eYtvlmCHHyzBLmp\nn4hew2l1nhLM85V11ydv8c0S5OCbJchNfXNzPkHe4pslyME3S5CDb5YgB98sQYUO7mUDZv4c+AC4\nGhFNScuBZ4FR4BzwYERcqqZMMyvTXLb4d0fEWEQ0s/lHgEMRsRY4lM2b2QLQT1N/C7Avm94HPNB/\nOWZWh6LBD+CHkl6TtCNbtjIiLmbT7wIrS6/OzCpR9AKeuyLigqTfBA5K+nH+yYgISTPeD5r9UOwA\nuPnmm/sq1szKUWiLHxEXsscJ4AXaw2O/J2kVQPY40eW9eyOiGRHNRqNRTtVm1pdZgy/pI5I+dm0a\n+CxwEngR2Jq9bCtwoKoizaxcRZr6K4EXssEZFgP/EBHfl/Qq8Jyk7cDbwIPVlWlmZZo1+BHxFrB+\nhuX/CdxbRVFmVi1fuWeWIAffLEEOvlmCHHyzBDn4Zgly8M0S5OCbJcjBN0uQg2+WIAffLEEOvlmC\nHHyzBDn4Zgly8M0S5OCbJcjBN0uQg2+WIAffLEEOvlmCHHyzBDn4Zgly8M0S5OCbJcjBN0uQg2+W\noELBlzQi6TuSfizpjKQ7JC2XdFDSm9njsqqLNbNyFN3i7wG+HxG30h5O6wzwCHAoItYCh7J5M1sA\nioyWuxT4DPAUQET8b0RcBrYA+7KX7QMeqKpIMytXkS3+GmAS+HtJxyT9XTZc9sqIuJi95l3ao+qa\n2QJQJPiLgU8BT0bEBuB/6GjWR0QAMdObJe2Q1JLUmpyc7LdeMytBkeCfB85HxNFs/ju0fwjek7QK\nIHucmOnNEbE3IpoR0Ww0GmXUbGZ9mjX4EfEu8I6kT2aL7gVOAy8CW7NlW4EDlVRoZqVbXPB1fww8\nI2kJ8Bbwh7R/NJ6TtB14G3iwmhLNrGyFgh8Rx4HmDE/dW245ZlYHX7lnliAH3yxBDr5Zghx8swQ5\n+GYJcvDNEuTgmyVI7cvsa1qZNEn7Yp8VwPu1rXhmw1ADuI5OrmO6udbxWxEx67XxtQZ/aqVSKyJm\nuiAoqRpch+sYVB1u6pslyME3S9Cggr93QOvNG4YawHV0ch3TVVLHQPbxzWyw3NQ3S1CtwZe0WdIb\nks5Kqq1XXklPS5qQdDK3rPbuwSXdJOmwpNOSTkkaH0Qtkj4k6RVJJ7I6vpYtXyPpaPb9PJv1v1A5\nSYuy/hxfGlQdks5J+pGk45Ja2bJB/I3U0pV9bcGXtAj4a+B3gXXAQ5LW1bT6bwKbO5YNonvwq8CX\nI2IdcDvwcPb/oO5afgHcExHrgTFgs6TbgceB3RFxC3AJ2F5xHdeM0+6y/ZpB1XF3RIzlTp8N4m+k\nnq7sI6KWf8AdwA9y848Cj9a4/lHgZG7+DWBVNr0KeKOuWnI1HADuG2QtwG8A/w58mvaFIotn+r4q\nXP/q7I/5HuAlQAOq4xywomNZrd8LsBT4D7Jjb1XWUWdT/0bgndz8+WzZoAy0e3BJo8AG4Oggasma\n18dpd5J6EPgpcDkirmYvqev7eQL4CvCrbP7jA6ojgB9Kek3SjmxZ3d9LbV3Z++AevbsHr4KkjwLP\nAzsj4meDqCUiPoiIMdpb3NuAW6teZydJ9wMTEfFa3euewV0R8Snau6IPS/pM/smavpe+urKfizqD\nfwG4KTe/Ols2KIW6By+bpBtoh/6ZiPjuIGsBiPaoSIdpN6lHJF3rh7GO7+dO4POSzgH7aTf39wyg\nDiLiQvY4AbxA+8ew7u+lr67s56LO4L8KrM2O2C4Bvki7i+5Bqb17cEmiPRTZmYj4xqBqkdSQNJJN\nf5j2cYYztH8AvlBXHRHxaESsjohR2n8P/xwRv193HZI+Iulj16aBzwInqfl7iTq7sq/6oEnHQYrP\nAT+hvT/55zWu91vAReCXtH9Vt9PelzwEvAn8E7C8hjruot1Mex04nv37XN21AL8NHMvqOAn8Rbb8\nE8ArwFng28Cv1/gdbQJeGkQd2fpOZP9OXfvbHNDfyBjQyr6bfwSWVVGHr9wzS5AP7pklyME3S5CD\nb5YgB98sQQ6+WYIcfLMEOfhmCXLwzRL0f2b5ckKh7/VIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9eebd37208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(images[100],\"gray\")\n",
    "plt.show()\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(base_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2368,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2376,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2370,\n",
       " 2375,\n",
       " 2375,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2376,\n",
       " 0,\n",
       " 2366,\n",
       " 2375,\n",
       " 0,\n",
       " 2380,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 2379,\n",
       " 2375,\n",
       " 2376,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2380,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 2380,\n",
       " 0,\n",
       " 0,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 2380,\n",
       " 0,\n",
       " 2380,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2376,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 2366,\n",
       " 2370,\n",
       " 2368,\n",
       " 0,\n",
       " 2370,\n",
       " 2367,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2306,\n",
       " 2367,\n",
       " 2366,\n",
       " 0,\n",
       " 2375,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2306,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 2376,\n",
       " 2368,\n",
       " 0,\n",
       " 2379,\n",
       " 2368,\n",
       " 2367,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2367,\n",
       " 2306,\n",
       " 2369,\n",
       " 2366,\n",
       " 2368,\n",
       " 2376,\n",
       " 2366,\n",
       " 2370,\n",
       " 2370,\n",
       " 2363,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2369,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2367,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 2379,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 2366,\n",
       " 2367,\n",
       " 2366,\n",
       " 2368,\n",
       " 2366,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 2375,\n",
       " 2306,\n",
       " 2367,\n",
       " 2366,\n",
       " 0,\n",
       " 2375,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 2376,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2369,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2366,\n",
       " 2376,\n",
       " 2366,\n",
       " 0,\n",
       " 2368,\n",
       " 0,\n",
       " 2306,\n",
       " 2366,\n",
       " 0,\n",
       " 2379,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 2375,\n",
       " 2376,\n",
       " 2366,\n",
       " 2364,\n",
       " 0,\n",
       " 0,\n",
       " 2376,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 2376,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2380,\n",
       " 2366,\n",
       " 2366,\n",
       " 2375,\n",
       " 2376,\n",
       " 0,\n",
       " 2379,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2370,\n",
       " 0,\n",
       " 2366,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 2367,\n",
       " 2379,\n",
       " 0,\n",
       " 2379,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2376,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 2367,\n",
       " 2380,\n",
       " 2306,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2306,\n",
       " 0,\n",
       " 2375,\n",
       " 2367,\n",
       " 2366,\n",
       " 2306,\n",
       " 2376,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2306,\n",
       " 2306,\n",
       " 2366,\n",
       " 2369,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 2376,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2367,\n",
       " 2375,\n",
       " 2366,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 2379,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2366,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2375,\n",
       " 2368,\n",
       " 0,\n",
       " 2368,\n",
       " 2369,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2366,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 2368,\n",
       " 2364,\n",
       " 0,\n",
       " 0,\n",
       " 2376,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 2306,\n",
       " 2367,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 2364,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2367,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2380,\n",
       " 2379,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 2367,\n",
       " 2306,\n",
       " 2367,\n",
       " 2375,\n",
       " 2368,\n",
       " 2366,\n",
       " 0,\n",
       " 2367,\n",
       " 2380,\n",
       " 0,\n",
       " 2390,\n",
       " 2375,\n",
       " 2368,\n",
       " 0,\n",
       " 2380,\n",
       " 2306,\n",
       " 0,\n",
       " 2376,\n",
       " 2367,\n",
       " 2306,\n",
       " 0,\n",
       " 2368,\n",
       " 2370,\n",
       " 0,\n",
       " 2380,\n",
       " 2369,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2380,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2379,\n",
       " 2366,\n",
       " 0,\n",
       " 2306,\n",
       " 2375,\n",
       " 0,\n",
       " 2306,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2366,\n",
       " 2366,\n",
       " 2306,\n",
       " 2366,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2376,\n",
       " 2366,\n",
       " 2306,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 2376,\n",
       " 2380,\n",
       " 2367,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2362,\n",
       " 2367,\n",
       " 2380,\n",
       " 0,\n",
       " 2380,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2369,\n",
       " 2382,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2367,\n",
       " 2366,\n",
       " 2366,\n",
       " 2370,\n",
       " 0,\n",
       " 2306,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 0,\n",
       " 2376,\n",
       " 2367,\n",
       " 2367,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2375,\n",
       " 2366,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 2370,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2366,\n",
       " 2306,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 2376,\n",
       " 2380,\n",
       " 0,\n",
       " 2306,\n",
       " 2369,\n",
       " 0,\n",
       " 2376,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2376,\n",
       " 0,\n",
       " 2370,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2366,\n",
       " 2368,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2367,\n",
       " 2379,\n",
       " 2380,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 2368,\n",
       " 0,\n",
       " 2364,\n",
       " 2368,\n",
       " 2306,\n",
       " 2367,\n",
       " 0,\n",
       " 2368,\n",
       " 2375,\n",
       " 2379,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 2369,\n",
       " 2367,\n",
       " 2366,\n",
       " 0,\n",
       " 2363,\n",
       " 2380,\n",
       " 0,\n",
       " 0,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 2380,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 2367,\n",
       " 2366,\n",
       " 2369,\n",
       " 2366,\n",
       " 2306,\n",
       " 2375,\n",
       " 2366,\n",
       " 2375,\n",
       " 2366,\n",
       " 2367,\n",
       " 2367,\n",
       " 2375,\n",
       " 2367,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 2367,\n",
       " 2366,\n",
       " 2366,\n",
       " 2379,\n",
       " 0,\n",
       " 2379,\n",
       " 2366,\n",
       " 2366,\n",
       " 2367,\n",
       " 2379,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2375,\n",
       " 2364,\n",
       " 0,\n",
       " 2376,\n",
       " 2376,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2369,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 2370,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2368,\n",
       " 2380,\n",
       " 2366,\n",
       " 2366,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 2366,\n",
       " 2366,\n",
       " 0,\n",
       " 2306,\n",
       " 2367,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2367,\n",
       " 2375,\n",
       " 2375,\n",
       " 2306,\n",
       " 2368,\n",
       " 2380,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 2366,\n",
       " 2375,\n",
       " 0,\n",
       " 2367,\n",
       " 2367,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 2376,\n",
       " 2306,\n",
       " 2375,\n",
       " 0,\n",
       " 2375,\n",
       " 2367,\n",
       " 2368,\n",
       " 2366,\n",
       " 2379,\n",
       " 2366,\n",
       " 2375,\n",
       " 0,\n",
       " 2367,\n",
       " 2367,\n",
       " 0,\n",
       " 2306,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 2379,\n",
       " 0,\n",
       " 2367,\n",
       " 2362,\n",
       " 2366,\n",
       " 0,\n",
       " 2367,\n",
       " 2366,\n",
       " 2366,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2367,\n",
       " 2366,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 2376,\n",
       " 0,\n",
       " 2376,\n",
       " 0,\n",
       " 2306,\n",
       " 0,\n",
       " 2380,\n",
       " 0,\n",
       " 2366,\n",
       " 2379,\n",
       " 2379,\n",
       " 2370,\n",
       " 2368,\n",
       " 2379,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2379,\n",
       " 2366,\n",
       " 2362,\n",
       " 0,\n",
       " 2376,\n",
       " 2367,\n",
       " 2306,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 2368,\n",
       " 2376,\n",
       " 2366,\n",
       " 2306,\n",
       " 2367,\n",
       " 2367,\n",
       " 0,\n",
       " 2366,\n",
       " 2367,\n",
       " 2380,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2375,\n",
       " 2368,\n",
       " 2367,\n",
       " 2366,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 2369,\n",
       " 0,\n",
       " 0,\n",
       " 2380,\n",
       " 0,\n",
       " 2366,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2375,\n",
       " 2368,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2376,\n",
       " 0,\n",
       " 0,\n",
       " 2372,\n",
       " 2306,\n",
       " 2368,\n",
       " 0,\n",
       " 2370,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 2367,\n",
       " 2369,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 2366,\n",
       " 0,\n",
       " 2387,\n",
       " 2367,\n",
       " 2366,\n",
       " 2363,\n",
       " 2367,\n",
       " 0,\n",
       " 2366,\n",
       " 2367,\n",
       " 2367,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2306,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 0,\n",
       " 2366,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2367,\n",
       " 2367,\n",
       " 2366,\n",
       " 2367,\n",
       " 2366,\n",
       " 2370,\n",
       " 2380,\n",
       " 2364,\n",
       " 2366,\n",
       " 2366,\n",
       " 2375,\n",
       " 0,\n",
       " 2375,\n",
       " 2369,\n",
       " 2368,\n",
       " 0,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2379,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 2366,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2306,\n",
       " 2364,\n",
       " 2366,\n",
       " 2375,\n",
       " 2379,\n",
       " 2306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2375,\n",
       " 2367,\n",
       " 0,\n",
       " 2376,\n",
       " 2366,\n",
       " 2379,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2366,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 2375,\n",
       " 0,\n",
       " 0,\n",
       " 2368,\n",
       " 2368,\n",
       " 2367,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matra_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "X = np.reshape(np.array(images), (-1, 64*64))\n",
    "y = np.array(base_class)\n",
    "\n",
    "df = pd.DataFrame(X, y)\n",
    "df['LABEL'] = df.index\n",
    "\n",
    "df_no_label = df.drop(columns='LABEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...   4086  \\\n",
       "2325   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...    1.0   \n",
       "2350   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...    1.0   \n",
       "2347   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...    1.0   \n",
       "2360   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...    1.0   \n",
       "2312   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...    1.0   \n",
       "\n",
       "      4087  4088  4089  4090  4091  4092  4093  4094  4095  \n",
       "2325   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "2350   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "2347   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "2360   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "2312   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
       "\n",
       "[5 rows x 4096 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(base_class)\n",
    "y_labeled = le.transform(base_class)\n",
    "y_train = np_utils.to_categorical(y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1895, 53)\n",
      "(1895, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "X_train = np.reshape(images,(-1,64,64,1))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...    4087  4088  \\\n",
       "2325  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...     1.0   1.0   \n",
       "2350  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...     1.0   1.0   \n",
       "2347  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...     1.0   1.0   \n",
       "2360  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...     1.0   1.0   \n",
       "2312  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...     1.0   1.0   \n",
       "\n",
       "      4089  4090  4091  4092  4093  4094  4095  LABEL  \n",
       "2325   1.0   1.0   1.0   1.0   1.0   1.0   1.0   2325  \n",
       "2350   1.0   1.0   1.0   1.0   1.0   1.0   1.0   2350  \n",
       "2347   1.0   1.0   1.0   1.0   1.0   1.0   1.0   2347  \n",
       "2360   1.0   1.0   1.0   1.0   1.0   1.0   1.0   2360  \n",
       "2312   1.0   1.0   1.0   1.0   1.0   1.0   1.0   2312  \n",
       "\n",
       "[5 rows x 4097 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consonant Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Conv2D,Flatten,MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16,(2,2),input_shape=(X_train[0].shape),activation = 'relu'))\n",
    "model.add(Conv2D(32,(2,2),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(16,(2,2),activation = 'relu'))\n",
    "model.add(Conv2D(32,(2,2),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64,(2,2),activation = 'relu'))\n",
    "model.add(Conv2D(64,(2,2),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(base_class)),activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_87 (Conv2D)           (None, 63, 63, 16)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_88 (Conv2D)           (None, 62, 62, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_89 (Conv2D)           (None, 30, 30, 16)        2064      \n",
      "_________________________________________________________________\n",
      "conv2d_90 (Conv2D)           (None, 29, 29, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_91 (Conv2D)           (None, 13, 13, 64)        8256      \n",
      "_________________________________________________________________\n",
      "conv2d_92 (Conv2D)           (None, 12, 12, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 53)                13621     \n",
      "=================================================================\n",
      "Total params: 634,709\n",
      "Trainable params: 634,709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_score(theta):\n",
    "    def score(y_true, y_pred):\n",
    "\n",
    "        y_thresh = K.cast(K.greater(y_pred,theta),K.floatx())\n",
    "\n",
    "        true_pos =  K.sum(y_true * y_thresh)\n",
    "        false_pos = K.sum(y_true * (1. - y_thresh))\n",
    "        false_neg = K.sum((1. - y_true) * y_thresh)\n",
    "\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        \n",
    "        f1_score_val = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score_val\n",
    "    return score\n",
    "\n",
    "def custom_metric(y_true, y_pred):\n",
    "    return K.cast(K.equal(y_true,\n",
    "                          K.round(y_pred)),\n",
    "                  K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='consonantweights.best.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1326 samples, validate on 569 samples\n",
      "Epoch 1/30\n",
      "1326/1326 [==============================] - 13s 9ms/step - loss: 3.6601 - score: nan - categorical_accuracy: 0.0686 - val_loss: 3.3715 - val_score: nan - val_categorical_accuracy: 0.0914\n",
      "Epoch 2/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 3.4214 - score: nan - categorical_accuracy: 0.0973 - val_loss: 3.2456 - val_score: nan - val_categorical_accuracy: 0.1476\n",
      "Epoch 3/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 3.1511 - score: nan - categorical_accuracy: 0.1780 - val_loss: 2.8888 - val_score: nan - val_categorical_accuracy: 0.2039\n",
      "Epoch 4/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 2.6701 - score: nan - categorical_accuracy: 0.3092 - val_loss: 2.3666 - val_score: nan - val_categorical_accuracy: 0.3726\n",
      "Epoch 5/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 2.2681 - score: 0.2560 - categorical_accuracy: 0.4148 - val_loss: 2.1950 - val_score: 0.3037 - val_categorical_accuracy: 0.4148\n",
      "Epoch 6/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 2.0173 - score: 0.3441 - categorical_accuracy: 0.4857 - val_loss: 1.9392 - val_score: 0.3767 - val_categorical_accuracy: 0.5114\n",
      "Epoch 7/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 1.7063 - score: 0.4465 - categorical_accuracy: 0.5581 - val_loss: 1.7189 - val_score: 0.4451 - val_categorical_accuracy: 0.5712\n",
      "Epoch 8/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 1.4921 - score: 0.5270 - categorical_accuracy: 0.6305 - val_loss: 1.5811 - val_score: 0.5147 - val_categorical_accuracy: 0.6327\n",
      "Epoch 9/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 1.2666 - score: 0.6029 - categorical_accuracy: 0.6810 - val_loss: 1.4592 - val_score: 0.5511 - val_categorical_accuracy: 0.6344\n",
      "Epoch 10/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 1.1016 - score: 0.6384 - categorical_accuracy: 0.7315 - val_loss: 1.3678 - val_score: 0.6058 - val_categorical_accuracy: 0.6626\n",
      "Epoch 11/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.9531 - score: 0.6976 - categorical_accuracy: 0.7692 - val_loss: 1.2461 - val_score: 0.6665 - val_categorical_accuracy: 0.6907\n",
      "Epoch 12/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.7854 - score: 0.7595 - categorical_accuracy: 0.8160 - val_loss: 1.2030 - val_score: 0.6735 - val_categorical_accuracy: 0.7118\n",
      "Epoch 13/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.6575 - score: 0.7896 - categorical_accuracy: 0.8529 - val_loss: 1.0991 - val_score: 0.6999 - val_categorical_accuracy: 0.7276\n",
      "Epoch 14/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.5524 - score: 0.8373 - categorical_accuracy: 0.8748 - val_loss: 1.0773 - val_score: 0.7102 - val_categorical_accuracy: 0.7399\n",
      "Epoch 15/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.4826 - score: 0.8570 - categorical_accuracy: 0.9027 - val_loss: 1.0597 - val_score: 0.7191 - val_categorical_accuracy: 0.7522\n",
      "Epoch 16/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.4123 - score: 0.8894 - categorical_accuracy: 0.9276 - val_loss: 1.0061 - val_score: 0.7342 - val_categorical_accuracy: 0.7645\n",
      "Epoch 17/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.3727 - score: 0.8974 - categorical_accuracy: 0.9321 - val_loss: 0.9841 - val_score: 0.7461 - val_categorical_accuracy: 0.7487\n",
      "Epoch 18/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.2920 - score: 0.9270 - categorical_accuracy: 0.9517 - val_loss: 0.9984 - val_score: 0.7362 - val_categorical_accuracy: 0.7540\n",
      "Epoch 19/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.2755 - score: 0.9373 - categorical_accuracy: 0.9585 - val_loss: 0.9727 - val_score: 0.7476 - val_categorical_accuracy: 0.7540\n",
      "Epoch 20/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.2315 - score: 0.9467 - categorical_accuracy: 0.9638 - val_loss: 0.9342 - val_score: 0.7583 - val_categorical_accuracy: 0.7680\n",
      "Epoch 21/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.2064 - score: 0.9554 - categorical_accuracy: 0.9713 - val_loss: 0.9575 - val_score: 0.7593 - val_categorical_accuracy: 0.7610\n",
      "Epoch 22/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.1930 - score: 0.9602 - categorical_accuracy: 0.9721 - val_loss: 0.9195 - val_score: 0.7705 - val_categorical_accuracy: 0.7663\n",
      "Epoch 23/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.1715 - score: 0.9671 - categorical_accuracy: 0.9774 - val_loss: 0.9063 - val_score: 0.7674 - val_categorical_accuracy: 0.7715\n",
      "Epoch 24/30\n",
      "1326/1326 [==============================] - 13s 9ms/step - loss: 0.1550 - score: 0.9685 - categorical_accuracy: 0.9781 - val_loss: 0.9335 - val_score: 0.7657 - val_categorical_accuracy: 0.7680\n",
      "Epoch 25/30\n",
      "1326/1326 [==============================] - 13s 9ms/step - loss: 0.1243 - score: 0.9795 - categorical_accuracy: 0.9910 - val_loss: 0.9095 - val_score: 0.7692 - val_categorical_accuracy: 0.7680\n",
      "Epoch 26/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.1201 - score: 0.9818 - categorical_accuracy: 0.9864 - val_loss: 0.9173 - val_score: 0.7636 - val_categorical_accuracy: 0.7715\n",
      "Epoch 27/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.1011 - score: 0.9841 - categorical_accuracy: 0.9910 - val_loss: 0.9184 - val_score: 0.7763 - val_categorical_accuracy: 0.7786\n",
      "Epoch 28/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.0926 - score: 0.9885 - categorical_accuracy: 0.9947 - val_loss: 0.9023 - val_score: 0.7818 - val_categorical_accuracy: 0.7768\n",
      "Epoch 29/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.0820 - score: 0.9924 - categorical_accuracy: 0.9955 - val_loss: 0.8968 - val_score: 0.7766 - val_categorical_accuracy: 0.7803\n",
      "Epoch 30/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.0696 - score: 0.9950 - categorical_accuracy: 0.9985 - val_loss: 0.9056 - val_score: 0.7864 - val_categorical_accuracy: 0.7838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9eea271eb8>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matra Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_matra = np.array(matra_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le_matra = LabelEncoder()\n",
    "le_matra.fit(matra_class)\n",
    "y_matra_labeled = le_matra.transform(matra_class)\n",
    "y_matra_train = np_utils.to_categorical(y_matra_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_matra_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16,(2,2),input_shape=(X_train[0].shape),activation = 'relu'))\n",
    "model.add(Conv2D(32,(2,2),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(16,(2,2),activation = 'relu'))\n",
    "model.add(Conv2D(32,(2,2),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64,(2,2),activation = 'relu'))\n",
    "model.add(Conv2D(64,(2,2),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(len(set(matra_class)),activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1326 samples, validate on 569 samples\n",
      "Epoch 1/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 1.3075 - score: 0.5950 - categorical_accuracy: 0.6214 - val_loss: 0.9611 - val_score: 0.6733 - val_categorical_accuracy: 0.7153\n",
      "Epoch 2/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.9681 - score: 0.7049 - categorical_accuracy: 0.7112 - val_loss: 0.8561 - val_score: 0.7063 - val_categorical_accuracy: 0.7469\n",
      "Epoch 3/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.8159 - score: 0.7419 - categorical_accuracy: 0.7572 - val_loss: 0.6918 - val_score: 0.7869 - val_categorical_accuracy: 0.7838\n",
      "Epoch 4/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.6878 - score: 0.7803 - categorical_accuracy: 0.7949 - val_loss: 0.6331 - val_score: 0.7858 - val_categorical_accuracy: 0.7996\n",
      "Epoch 5/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.5663 - score: 0.8183 - categorical_accuracy: 0.8318 - val_loss: 0.5498 - val_score: 0.8357 - val_categorical_accuracy: 0.8418\n",
      "Epoch 6/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.4930 - score: 0.8412 - categorical_accuracy: 0.8552 - val_loss: 0.5273 - val_score: 0.8400 - val_categorical_accuracy: 0.8489\n",
      "Epoch 7/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.4389 - score: 0.8666 - categorical_accuracy: 0.8718 - val_loss: 0.5054 - val_score: 0.8495 - val_categorical_accuracy: 0.8559\n",
      "Epoch 8/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.3761 - score: 0.8894 - categorical_accuracy: 0.8891 - val_loss: 0.5118 - val_score: 0.8514 - val_categorical_accuracy: 0.8559\n",
      "Epoch 9/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.3140 - score: 0.9026 - categorical_accuracy: 0.9095 - val_loss: 0.4754 - val_score: 0.8617 - val_categorical_accuracy: 0.8629\n",
      "Epoch 10/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.2694 - score: 0.9226 - categorical_accuracy: 0.9231 - val_loss: 0.4766 - val_score: 0.8569 - val_categorical_accuracy: 0.8576\n",
      "Epoch 11/30\n",
      "1326/1326 [==============================] - 13s 9ms/step - loss: 0.2211 - score: 0.9455 - categorical_accuracy: 0.9502 - val_loss: 0.4734 - val_score: 0.8664 - val_categorical_accuracy: 0.8717\n",
      "Epoch 12/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.2061 - score: 0.9459 - categorical_accuracy: 0.9472 - val_loss: 0.4589 - val_score: 0.8719 - val_categorical_accuracy: 0.8699\n",
      "Epoch 13/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.1855 - score: 0.9546 - categorical_accuracy: 0.9532 - val_loss: 0.4895 - val_score: 0.8499 - val_categorical_accuracy: 0.8664\n",
      "Epoch 14/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.1508 - score: 0.9673 - categorical_accuracy: 0.9683 - val_loss: 0.4560 - val_score: 0.8693 - val_categorical_accuracy: 0.8752\n",
      "Epoch 15/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.1394 - score: 0.9650 - categorical_accuracy: 0.9646 - val_loss: 0.4623 - val_score: 0.8628 - val_categorical_accuracy: 0.8682\n",
      "Epoch 16/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.1125 - score: 0.9741 - categorical_accuracy: 0.9766 - val_loss: 0.4849 - val_score: 0.8670 - val_categorical_accuracy: 0.8647\n",
      "Epoch 17/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.0898 - score: 0.9787 - categorical_accuracy: 0.9819 - val_loss: 0.4709 - val_score: 0.8702 - val_categorical_accuracy: 0.8699\n",
      "Epoch 18/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.0829 - score: 0.9840 - categorical_accuracy: 0.9864 - val_loss: 0.4839 - val_score: 0.8593 - val_categorical_accuracy: 0.8664\n",
      "Epoch 19/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.0761 - score: 0.9856 - categorical_accuracy: 0.9864 - val_loss: 0.4624 - val_score: 0.8626 - val_categorical_accuracy: 0.8682\n",
      "Epoch 20/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.0603 - score: 0.9882 - categorical_accuracy: 0.9879 - val_loss: 0.4762 - val_score: 0.8718 - val_categorical_accuracy: 0.8717\n",
      "Epoch 21/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.0505 - score: 0.9909 - categorical_accuracy: 0.9925 - val_loss: 0.4824 - val_score: 0.8660 - val_categorical_accuracy: 0.8664\n",
      "Epoch 22/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.0400 - score: 0.9966 - categorical_accuracy: 0.9962 - val_loss: 0.4948 - val_score: 0.8658 - val_categorical_accuracy: 0.8699\n",
      "Epoch 23/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.0439 - score: 0.9940 - categorical_accuracy: 0.9947 - val_loss: 0.4979 - val_score: 0.8790 - val_categorical_accuracy: 0.8770\n",
      "Epoch 24/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.0353 - score: 0.9940 - categorical_accuracy: 0.9940 - val_loss: 0.4768 - val_score: 0.8714 - val_categorical_accuracy: 0.8717\n",
      "Epoch 25/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.0332 - score: 0.9935 - categorical_accuracy: 0.9947 - val_loss: 0.4935 - val_score: 0.8693 - val_categorical_accuracy: 0.8752\n",
      "Epoch 26/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.0294 - score: 0.9966 - categorical_accuracy: 0.9962 - val_loss: 0.4843 - val_score: 0.8768 - val_categorical_accuracy: 0.8717\n",
      "Epoch 27/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.0262 - score: 0.9970 - categorical_accuracy: 0.9977 - val_loss: 0.5235 - val_score: 0.8666 - val_categorical_accuracy: 0.8664\n",
      "Epoch 28/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.0196 - score: 0.9981 - categorical_accuracy: 0.9985 - val_loss: 0.4926 - val_score: 0.8661 - val_categorical_accuracy: 0.8682\n",
      "Epoch 29/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.0180 - score: 0.9985 - categorical_accuracy: 0.9985 - val_loss: 0.5029 - val_score: 0.8684 - val_categorical_accuracy: 0.8664\n",
      "Epoch 30/30\n",
      "1326/1326 [==============================] - 12s 9ms/step - loss: 0.0161 - score: 0.9989 - categorical_accuracy: 0.9992 - val_loss: 0.5030 - val_score: 0.8703 - val_categorical_accuracy: 0.8770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ee80e8c88>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_matra_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_dot = np.array(dot_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_dot = LabelEncoder()\n",
    "le_dot.fit(dot_class)\n",
    "y_dot_labeled = le_dot.transform(dot_class)\n",
    "y_dot_train = np_utils.to_categorical(y_dot_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_dot_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16,(2,2),input_shape=(X_train[0].shape),activation = 'relu'))\n",
    "model.add(Conv2D(32,(2,2),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64,(2,2),activation = 'relu'))\n",
    "model.add(Conv2D(64,(2,2),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(len(set(dot_class)),activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1326 samples, validate on 569 samples\n",
      "Epoch 1/20\n",
      "1326/1326 [==============================] - 21s 16ms/step - loss: 0.1880 - score: 0.9612 - categorical_accuracy: 0.9668 - val_loss: 0.1383 - val_score: 0.9772 - val_categorical_accuracy: 0.9772\n",
      "Epoch 2/20\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.1060 - score: 0.9834 - categorical_accuracy: 0.9834 - val_loss: 0.1286 - val_score: 0.9772 - val_categorical_accuracy: 0.9772\n",
      "Epoch 3/20\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.1073 - score: 0.9834 - categorical_accuracy: 0.9834 - val_loss: 0.1299 - val_score: 0.9772 - val_categorical_accuracy: 0.9772\n",
      "Epoch 4/20\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.1018 - score: 0.9834 - categorical_accuracy: 0.9834 - val_loss: 0.1452 - val_score: 0.9772 - val_categorical_accuracy: 0.9772\n",
      "Epoch 5/20\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.0877 - score: 0.9834 - categorical_accuracy: 0.9834 - val_loss: 0.1394 - val_score: 0.9772 - val_categorical_accuracy: 0.9772\n",
      "Epoch 6/20\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.0784 - score: 0.9838 - categorical_accuracy: 0.9842 - val_loss: 0.1355 - val_score: 0.9772 - val_categorical_accuracy: 0.9772\n",
      "Epoch 7/20\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.0556 - score: 0.9849 - categorical_accuracy: 0.9849 - val_loss: 0.1362 - val_score: 0.9736 - val_categorical_accuracy: 0.9736\n",
      "Epoch 8/20\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.0471 - score: 0.9864 - categorical_accuracy: 0.9864 - val_loss: 0.1391 - val_score: 0.9772 - val_categorical_accuracy: 0.9772\n",
      "Epoch 9/20\n",
      "1326/1326 [==============================] - 22s 16ms/step - loss: 0.0334 - score: 0.9910 - categorical_accuracy: 0.9910 - val_loss: 0.1797 - val_score: 0.9772 - val_categorical_accuracy: 0.9772\n",
      "Epoch 10/20\n",
      " 544/1326 [===========>..................] - ETA: 11s - loss: 0.0265 - score: 0.9890 - categorical_accuracy: 0.9890"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-7627968737f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_dot_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_dot_train,epochs=3,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
