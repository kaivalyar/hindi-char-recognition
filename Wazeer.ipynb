{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io, morphology, img_as_bool, segmentation\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.ndimage.morphology import binary_fill_holes\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_resize(img):\n",
    "    top = int((224 - img.shape[0])/2)\n",
    "    left = int((224 - img.shape[1])/2)\n",
    "    bottom = 224 - img.shape[0] - top\n",
    "    right = 224 - img.shape[1] - left\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=255)\n",
    "    img = img/255.\n",
    "    img = cv2.resize(img, img_shape) #KADD\n",
    "    return img\n",
    "\n",
    "def skeletonize(img):\n",
    "    size = np.size(img)\n",
    "    skel = np.zeros(img.shape,np.uint8)\n",
    "    img = cv2.bitwise_not(img)\n",
    "#     element = cv2.getStructuringElement(cv2.MORPH_CROSS,(1,1))\n",
    "#     done = 0\n",
    "#     while( done < 1 ):\n",
    "#         eroded = cv2.erode(img,element)\n",
    "#         temp = cv2.dilate(eroded,element)\n",
    "#         temp = cv2.subtract(img,temp)\n",
    "#         skel = cv2.bitwise_or(skel,temp)\n",
    "#         img = eroded.copy()\n",
    "\n",
    "#         zeros = size - cv2.countNonZero(img)\n",
    "#         if zeros==size:#cv2.countNonZero(img) * 1 >= 0:#\n",
    "#             done += 1\n",
    "#     img = skel\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    erosion = cv2.erode(img,kernel,iterations = 2)\n",
    "    img = cv2.bitwise_not(erosion)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page0_15_8_2350_2362_2380.png\n",
      "100 Done\n",
      "page0_8_8_2330_2375_2379.png\n",
      "200 Done\n",
      "page0_9_15_2332_2375_2379.png\n",
      "300 Done\n",
      "page1_16_0_2325_2362_2380.png\n",
      "page1_16_13_2346_2362_2369.png\n",
      "400 Done\n",
      "page1_8_0_2357_2364_2366.png\n",
      "page1_9_15_2351_2364_2366.png\n",
      "500 Done\n",
      "600 Done\n",
      "page2_4_1_2357_2364_2366.png\n",
      "page2_4_20_2346_2366_2390.png\n",
      "700 Done\n",
      "800 Done\n",
      "page3_18_4_2332_2366_2379.png\n",
      "900 Done\n",
      "ohno page3_6_0_2327_2417.png\n",
      "1000 Done\n",
      "page4_15_2_2357_2364_2376.png\n",
      "1100 Done\n",
      "1200 Done\n",
      "page5_17_5_2325_2362_2376.png\n",
      "1300 Done\n",
      "page5_3_15_2351_2364_2366.png\n",
      "1400 Done\n",
      "page5_7_8_2325_2367_2388.png\n",
      "page6_10_6_2325_2366_2380.png\n",
      "1500 Done\n",
      "page6_17_13_2351_2364_2366.png\n",
      "page6_1_1_2340_2375_2379.png\n",
      "1600 Done\n",
      "page6_6_9_2352_2362_2363.png\n",
      "1700 Done\n",
      "page7_15_14_2349_2362_2366.png\n",
      "ohno page7_16_10_2350_2417.png\n",
      "1800 Done\n",
      "page7_3_0_2319_2366_2367.png\n",
      "page7_3_13_2319_2366_2367.png\n"
     ]
    }
   ],
   "source": [
    "PATH = '../train_images_modified'\n",
    "images = []\n",
    "base_class = []\n",
    "matra_class = []\n",
    "dot_class = []\n",
    "total_class = []\n",
    "for filename in os.listdir(PATH):\n",
    "    if filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(PATH,filename),0)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        a,img = cv2.threshold(img,127,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)#KEDIT\n",
    "#         kernel = np.ones((5,5),np.uint8)\n",
    "#         img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#         plt.show()\n",
    "#        img = skeletonize(img)\n",
    "        img = pad_resize(img)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        char_arr = filename[:-4].split('_')[3:]\n",
    "        if(len(char_arr)>0):\n",
    "            images.append(img)\n",
    "            char_arr = [int(i) for i in char_arr]\n",
    "            total_class.append(char_arr)\n",
    "            base = [i for i in char_arr if (i>=2308 and i<=2361) or (i==2384) or (i>=2392 and i<=2401) or (i>=2404 and i!=2416 and i!=2417)]  \n",
    "            base_class.append(base[0])\n",
    "            matra = [i for i in char_arr if i>=2362 and i<=2391]\n",
    "            dot = [i for i in char_arr if i==2306 or i==2416]\n",
    "            if(len(matra)>1):\n",
    "                print(filename)\n",
    "            elif(len(matra)>0):\n",
    "                matra_class.append(matra[0])\n",
    "            else:\n",
    "                matra_class.append(0)\n",
    "            if len(dot)>0:\n",
    "                dot_class.append(1)\n",
    "            else:\n",
    "                dot_class.append(0)\n",
    "            if(len(matra)==0 and len(dot)==0 and len(char_arr)==2):\n",
    "                print(\"ohno \"+filename)\n",
    "#             if(len(char_arr)>1):          \n",
    "#                 matra_class.append(char_arr[1:])\n",
    "            if len(images)%100==0:\n",
    "                print(\"{} Done\".format(len(images)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nliAnvlmCnPhmCfp/HAp7z49tEzsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x138b68438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(images[10],\"gray\")\n",
    "plt.show()\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2392, 2353, 2310, 2384, 2415, 2414]"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(base_class))\n",
    "freq = {i:base_class.count(i) for i in base_class}\n",
    "[i for i in freq.keys() if freq[i]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "flat = img_shape[0]*img_shape[1]\n",
    "x_data = np.reshape(np.array(images), (-1, flat))\n",
    "y_data = np.array(base_class)\n",
    "\n",
    "df = pd.DataFrame(x_data, y_data)\n",
    "df['LABEL'] = df.index\n",
    "\n",
    "df_no_label = df.drop(columns='LABEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# #print(df.head())\n",
    "# #print(df_no_label)\n",
    "# #print(df['LABEL'])\n",
    "\n",
    "# X_train,X_val,y_train,y_val = train_test_split(df_no_label, df['LABEL'])\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_val.shape)\n",
    "\n",
    "# clf = RandomForestClassifier(max_features='auto', n_estimators=10, max_depth=20)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# scores = cross_val_score(clf, df_no_label, df['LABEL'], cv=3)\n",
    "# print(scores)\n",
    "# scores_f1 = f1_score(clf.predict(X_train),y_train,average='weighted')\n",
    "# print(scores_f1)\n",
    "# scores_f1 = f1_score(clf.predict(X_val),y_val,average='weighted')\n",
    "# print(scores_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(base_class)\n",
    "y_labeled = le.transform(base_class)\n",
    "y_train = np_utils.to_categorical(y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Dense(256,input_shape=(224*224,),activation = 'sigmoid'))\n",
    "# model.add(Dense(128,activation = 'sigmoid'))\n",
    "# model.add(Dense(128,activation = 'sigmoid'))\n",
    "# model.add(Dense(len(total_char_set),activation = 'sigmoid'))\n",
    "\n",
    "# print(model.summary())\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=[f1_score(theta=0.5),'accuracy'])\n",
    "# X_train = X_train.reshape((-1,224*224))\n",
    "# model.fit(X_train,y_train2,epochs=10,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1895, 53)\n",
      "(1895, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "x_train = np.reshape(images,(-1,img_shape[0],img_shape[1],1))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(base_class)),activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_68 (Conv2D)           (None, 62, 62, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 60, 60, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 26, 26, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 53)                13621     \n",
      "=================================================================\n",
      "Total params: 347,813\n",
      "Trainable params: 347,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_score(theta):\n",
    "    def score(y_true, y_pred):\n",
    "\n",
    "        y_thresh = K.cast(K.greater(y_pred,theta),K.floatx())\n",
    "\n",
    "        true_pos =  K.sum(y_true * y_thresh)\n",
    "        false_pos = K.sum(y_true * (1. - y_thresh))\n",
    "        false_neg = K.sum((1. - y_true) * y_thresh)\n",
    "\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        \n",
    "        f1_score_val = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score_val\n",
    "    return score\n",
    "\n",
    "def custom_metric(y_true, y_pred):\n",
    "    return K.cast(K.equal(y_true,\n",
    "                          K.round(y_pred)),\n",
    "                  K.floatx())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1326 samples, validate on 569 samples\n",
      "Epoch 1/30\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 3.5902 - score: nan - categorical_accuracy: 0.0762 - val_loss: 3.5273 - val_score: nan - val_categorical_accuracy: 0.0879\n",
      "Epoch 2/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 3.3950 - score: nan - categorical_accuracy: 0.0814 - val_loss: 3.4959 - val_score: nan - val_categorical_accuracy: 0.0879\n",
      "Epoch 3/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 3.2446 - score: nan - categorical_accuracy: 0.1350 - val_loss: 3.1465 - val_score: nan - val_categorical_accuracy: 0.2443\n",
      "Epoch 4/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 2.5088 - score: nan - categorical_accuracy: 0.3612 - val_loss: 2.5048 - val_score: 0.2881 - val_categorical_accuracy: 0.4200\n",
      "Epoch 5/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 1.9291 - score: 0.3741 - categorical_accuracy: 0.5287 - val_loss: 2.0987 - val_score: 0.3782 - val_categorical_accuracy: 0.5237\n",
      "Epoch 6/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 1.5149 - score: 0.5514 - categorical_accuracy: 0.6267 - val_loss: 1.8420 - val_score: 0.5010 - val_categorical_accuracy: 0.5800\n",
      "Epoch 7/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 1.2205 - score: 0.6570 - categorical_accuracy: 0.6998 - val_loss: 1.6371 - val_score: 0.5909 - val_categorical_accuracy: 0.6081\n",
      "Epoch 8/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 1.0055 - score: 0.7258 - categorical_accuracy: 0.7572 - val_loss: 1.4781 - val_score: 0.6508 - val_categorical_accuracy: 0.6380\n",
      "Epoch 9/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.8010 - score: 0.7812 - categorical_accuracy: 0.8062 - val_loss: 1.3784 - val_score: 0.6590 - val_categorical_accuracy: 0.6696\n",
      "Epoch 10/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.6597 - score: 0.8150 - categorical_accuracy: 0.8446 - val_loss: 1.2944 - val_score: 0.6750 - val_categorical_accuracy: 0.6942\n",
      "Epoch 11/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.5588 - score: 0.8467 - categorical_accuracy: 0.8680 - val_loss: 1.2614 - val_score: 0.6894 - val_categorical_accuracy: 0.6907\n",
      "Epoch 12/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.4910 - score: 0.8614 - categorical_accuracy: 0.8914 - val_loss: 1.2394 - val_score: 0.6877 - val_categorical_accuracy: 0.6977\n",
      "Epoch 13/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.4125 - score: 0.8906 - categorical_accuracy: 0.9087 - val_loss: 1.1911 - val_score: 0.7023 - val_categorical_accuracy: 0.7170\n",
      "Epoch 14/30\n",
      "1326/1326 [==============================] - 19s 14ms/step - loss: 0.3549 - score: 0.9007 - categorical_accuracy: 0.9231 - val_loss: 1.1524 - val_score: 0.7268 - val_categorical_accuracy: 0.7170\n",
      "Epoch 15/30\n",
      "1326/1326 [==============================] - 19s 14ms/step - loss: 0.2886 - score: 0.9265 - categorical_accuracy: 0.9374 - val_loss: 1.1097 - val_score: 0.7303 - val_categorical_accuracy: 0.7417\n",
      "Epoch 16/30\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.2593 - score: 0.9357 - categorical_accuracy: 0.9525 - val_loss: 1.1224 - val_score: 0.7354 - val_categorical_accuracy: 0.7452\n",
      "Epoch 17/30\n",
      "1326/1326 [==============================] - 21s 16ms/step - loss: 0.2198 - score: 0.9432 - categorical_accuracy: 0.9593 - val_loss: 1.1030 - val_score: 0.7458 - val_categorical_accuracy: 0.7329\n",
      "Epoch 18/30\n",
      "1326/1326 [==============================] - 22s 16ms/step - loss: 0.2057 - score: 0.9474 - categorical_accuracy: 0.9653 - val_loss: 1.0616 - val_score: 0.7581 - val_categorical_accuracy: 0.7487\n",
      "Epoch 19/30\n",
      "1326/1326 [==============================] - 18s 13ms/step - loss: 0.1769 - score: 0.9579 - categorical_accuracy: 0.9713 - val_loss: 1.0639 - val_score: 0.7506 - val_categorical_accuracy: 0.7417\n",
      "Epoch 20/30\n",
      "1326/1326 [==============================] - 21s 16ms/step - loss: 0.1556 - score: 0.9634 - categorical_accuracy: 0.9683 - val_loss: 1.0570 - val_score: 0.7632 - val_categorical_accuracy: 0.7645\n",
      "Epoch 21/30\n",
      "1326/1326 [==============================] - 22s 17ms/step - loss: 0.1319 - score: 0.9745 - categorical_accuracy: 0.9827 - val_loss: 1.0411 - val_score: 0.7550 - val_categorical_accuracy: 0.7452\n",
      "Epoch 22/30\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.1385 - score: 0.9711 - categorical_accuracy: 0.9781 - val_loss: 1.0595 - val_score: 0.7520 - val_categorical_accuracy: 0.7452\n",
      "Epoch 23/30\n",
      "1326/1326 [==============================] - 19s 15ms/step - loss: 0.1121 - score: 0.9762 - categorical_accuracy: 0.9827 - val_loss: 1.0478 - val_score: 0.7600 - val_categorical_accuracy: 0.7522\n",
      "Epoch 24/30\n",
      "1326/1326 [==============================] - 18s 13ms/step - loss: 0.0970 - score: 0.9823 - categorical_accuracy: 0.9864 - val_loss: 1.0366 - val_score: 0.7668 - val_categorical_accuracy: 0.7610\n",
      "Epoch 25/30\n",
      "1326/1326 [==============================] - 19s 14ms/step - loss: 0.0945 - score: 0.9835 - categorical_accuracy: 0.9849 - val_loss: 1.0436 - val_score: 0.7638 - val_categorical_accuracy: 0.7452\n",
      "Epoch 26/30\n",
      "1326/1326 [==============================] - 19s 14ms/step - loss: 0.0801 - score: 0.9847 - categorical_accuracy: 0.9910 - val_loss: 1.0502 - val_score: 0.7695 - val_categorical_accuracy: 0.7663\n",
      "Epoch 27/30\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.0723 - score: 0.9890 - categorical_accuracy: 0.9917 - val_loss: 1.0418 - val_score: 0.7675 - val_categorical_accuracy: 0.7540\n",
      "Epoch 28/30\n",
      "1326/1326 [==============================] - 18s 14ms/step - loss: 0.0662 - score: 0.9909 - categorical_accuracy: 0.9932 - val_loss: 1.0536 - val_score: 0.7618 - val_categorical_accuracy: 0.7627\n",
      "Epoch 29/30\n",
      "1326/1326 [==============================] - 18s 14ms/step - loss: 0.0598 - score: 0.9879 - categorical_accuracy: 0.9887 - val_loss: 1.0581 - val_score: 0.7670 - val_categorical_accuracy: 0.7522\n",
      "Epoch 30/30\n",
      "1326/1326 [==============================] - 18s 13ms/step - loss: 0.0557 - score: 0.9909 - categorical_accuracy: 0.9925 - val_loss: 1.0409 - val_score: 0.7752 - val_categorical_accuracy: 0.7715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1733aeeb8>"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOT CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1895, 2)"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np_utils.to_categorical(dot_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1326 samples, validate on 569 samples\n",
      "Epoch 1/30\n",
      "1326/1326 [==============================] - 21s 16ms/step - loss: 0.2794 - score: 0.9321 - categorical_accuracy: 0.9321 - val_loss: 0.2643 - val_score: 0.9315 - val_categorical_accuracy: 0.9315\n",
      "Epoch 2/30\n",
      "1326/1326 [==============================] - 18s 13ms/step - loss: 0.2343 - score: 0.9442 - categorical_accuracy: 0.9442 - val_loss: 0.2579 - val_score: 0.9315 - val_categorical_accuracy: 0.9315\n",
      "Epoch 3/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.2371 - score: 0.9449 - categorical_accuracy: 0.9449 - val_loss: 0.2497 - val_score: 0.9315 - val_categorical_accuracy: 0.9315\n",
      "Epoch 4/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.2312 - score: 0.9449 - categorical_accuracy: 0.9449 - val_loss: 0.2558 - val_score: 0.9315 - val_categorical_accuracy: 0.9315\n",
      "Epoch 5/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.2270 - score: 0.9449 - categorical_accuracy: 0.9449 - val_loss: 0.2758 - val_score: 0.9315 - val_categorical_accuracy: 0.9315\n",
      "Epoch 6/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.2039 - score: 0.9465 - categorical_accuracy: 0.9465 - val_loss: 0.1829 - val_score: 0.9297 - val_categorical_accuracy: 0.9297\n",
      "Epoch 7/30\n",
      "1326/1326 [==============================] - 18s 13ms/step - loss: 0.1303 - score: 0.9510 - categorical_accuracy: 0.9510 - val_loss: 0.1356 - val_score: 0.9420 - val_categorical_accuracy: 0.9420\n",
      "Epoch 8/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.1074 - score: 0.9653 - categorical_accuracy: 0.9653 - val_loss: 0.1438 - val_score: 0.9367 - val_categorical_accuracy: 0.9367\n",
      "Epoch 9/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.0907 - score: 0.9683 - categorical_accuracy: 0.9683 - val_loss: 0.1476 - val_score: 0.9350 - val_categorical_accuracy: 0.9350\n",
      "Epoch 10/30\n",
      "1326/1326 [==============================] - 19s 15ms/step - loss: 0.0795 - score: 0.9759 - categorical_accuracy: 0.9759 - val_loss: 0.1604 - val_score: 0.9279 - val_categorical_accuracy: 0.9279\n",
      "Epoch 11/30\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.0694 - score: 0.9766 - categorical_accuracy: 0.9766 - val_loss: 0.1713 - val_score: 0.9332 - val_categorical_accuracy: 0.9332\n",
      "Epoch 12/30\n",
      "1326/1326 [==============================] - 18s 14ms/step - loss: 0.0646 - score: 0.9759 - categorical_accuracy: 0.9759 - val_loss: 0.1892 - val_score: 0.9332 - val_categorical_accuracy: 0.9332\n",
      "Epoch 13/30\n",
      "1326/1326 [==============================] - 19s 14ms/step - loss: 0.0476 - score: 0.9857 - categorical_accuracy: 0.9857 - val_loss: 0.2203 - val_score: 0.9315 - val_categorical_accuracy: 0.9315\n",
      "Epoch 14/30\n",
      "1326/1326 [==============================] - 18s 14ms/step - loss: 0.0383 - score: 0.9879 - categorical_accuracy: 0.9879 - val_loss: 0.2196 - val_score: 0.9367 - val_categorical_accuracy: 0.9367\n",
      "Epoch 15/30\n",
      "1326/1326 [==============================] - 18s 13ms/step - loss: 0.0326 - score: 0.9925 - categorical_accuracy: 0.9925 - val_loss: 0.2439 - val_score: 0.9350 - val_categorical_accuracy: 0.9350\n",
      "Epoch 16/30\n",
      "1326/1326 [==============================] - 19s 14ms/step - loss: 0.0283 - score: 0.9932 - categorical_accuracy: 0.9932 - val_loss: 0.2540 - val_score: 0.9367 - val_categorical_accuracy: 0.9367\n",
      "Epoch 17/30\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.0235 - score: 0.9917 - categorical_accuracy: 0.9917 - val_loss: 0.2727 - val_score: 0.9385 - val_categorical_accuracy: 0.9385\n",
      "Epoch 18/30\n",
      "1326/1326 [==============================] - 21s 16ms/step - loss: 0.0166 - score: 0.9962 - categorical_accuracy: 0.9962 - val_loss: 0.3008 - val_score: 0.9332 - val_categorical_accuracy: 0.9332\n",
      "Epoch 19/30\n",
      "1326/1326 [==============================] - 18s 14ms/step - loss: 0.0319 - score: 0.9879 - categorical_accuracy: 0.9879 - val_loss: 0.2786 - val_score: 0.9367 - val_categorical_accuracy: 0.9367\n",
      "Epoch 20/30\n",
      "1326/1326 [==============================] - 20s 15ms/step - loss: 0.0132 - score: 0.9970 - categorical_accuracy: 0.9970 - val_loss: 0.3097 - val_score: 0.9350 - val_categorical_accuracy: 0.9350\n",
      "Epoch 21/30\n",
      "1184/1326 [=========================>....] - ETA: 1s - loss: 0.0111 - score: 0.9966 - categorical_accuracy: 0.9966"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-702-a750bd25186f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m               metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(dot_class)),activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATRA CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page0_15_8_2350_2362_2380.png\n",
      "100 Done\n",
      "page0_8_8_2330_2375_2379.png\n",
      "page0_9_15_2332_2375_2379.png\n",
      "200 Done\n",
      "page1_16_0_2325_2362_2380.png\n",
      "page1_16_13_2346_2362_2369.png\n",
      "300 Done\n",
      "400 Done\n",
      "page1_8_0_2357_2364_2366.png\n",
      "page1_9_15_2351_2364_2366.png\n",
      "500 Done\n",
      "600 Done\n",
      "page2_4_1_2357_2364_2366.png\n",
      "page2_4_20_2346_2366_2390.png\n",
      "700 Done\n",
      "800 Done\n",
      "page3_18_4_2332_2366_2379.png\n",
      "900 Done\n",
      "1000 Done\n",
      "page4_15_2_2357_2364_2376.png\n",
      "1100 Done\n",
      "1200 Done\n",
      "page5_17_5_2325_2362_2376.png\n",
      "1300 Done\n",
      "page5_3_15_2351_2364_2366.png\n",
      "page5_7_8_2325_2367_2388.png\n",
      "1400 Done\n",
      "page6_10_6_2325_2366_2380.png\n",
      "1500 Done\n",
      "page6_17_13_2351_2364_2366.png\n",
      "page6_1_1_2340_2375_2379.png\n",
      "1600 Done\n",
      "page6_6_9_2352_2362_2363.png\n",
      "1700 Done\n",
      "page7_15_14_2349_2362_2366.png\n",
      "page7_3_0_2319_2366_2367.png\n",
      "page7_3_13_2319_2366_2367.png\n",
      "1800 Done\n"
     ]
    }
   ],
   "source": [
    "PATH = '../train_images_modified'\n",
    "images = []\n",
    "matra_class = []\n",
    "for filename in os.listdir(PATH):\n",
    "    if filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(PATH,filename),0)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        a,img = cv2.threshold(img,127,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)#KEDIT\n",
    "#         kernel = np.ones((5,5),np.uint8)\n",
    "#         img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#         plt.show()\n",
    "#        img = skeletonize(img)\n",
    "        img = pad_resize(img)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        char_arr = filename[:-4].split('_')[3:]\n",
    "        if(len(char_arr)>0):\n",
    "            char_arr = [int(i) for i in char_arr]\n",
    "            matra = [i for i in char_arr if i>=2362 and i<=2391]\n",
    "            if(len(matra)>1):\n",
    "                print(filename)\n",
    "            elif(len(matra)>0):\n",
    "                images.append(img)\n",
    "                matra_class.append(matra[0])\n",
    "            else:\n",
    "                images.append(img)\n",
    "                matra_class.append('0')\n",
    "            if len(images)%100==0:\n",
    "                print(\"{} Done\".format(len(images)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1874, 17)\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(matra_class)\n",
    "y_labeled = le.transform(matra_class)\n",
    "y_train = np_utils.to_categorical(y_labeled)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_labeled)\n",
    "images = np.array(images)\n",
    "x_train = np.reshape(images,(-1,img_shape[0],img_shape[1],1))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1311 samples, validate on 563 samples\n",
      "Epoch 1/30\n",
      "1311/1311 [==============================] - 21s 16ms/step - loss: 0.1614 - score: nan - categorical_accuracy: 0.4584 - val_loss: 0.1295 - val_score: 0.4995 - val_categorical_accuracy: 0.5808\n",
      "Epoch 2/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.1135 - score: 0.5892 - categorical_accuracy: 0.6400 - val_loss: 0.0893 - val_score: 0.7151 - val_categorical_accuracy: 0.7140\n",
      "Epoch 3/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0767 - score: 0.7568 - categorical_accuracy: 0.7544 - val_loss: 0.0710 - val_score: 0.7553 - val_categorical_accuracy: 0.7762\n",
      "Epoch 4/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0649 - score: 0.7761 - categorical_accuracy: 0.7879 - val_loss: 0.0633 - val_score: 0.7989 - val_categorical_accuracy: 0.7886\n",
      "Epoch 5/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0514 - score: 0.8375 - categorical_accuracy: 0.8368 - val_loss: 0.0591 - val_score: 0.8148 - val_categorical_accuracy: 0.8082\n",
      "Epoch 6/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0448 - score: 0.8626 - categorical_accuracy: 0.8619 - val_loss: 0.0513 - val_score: 0.8446 - val_categorical_accuracy: 0.8544\n",
      "Epoch 7/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0378 - score: 0.8914 - categorical_accuracy: 0.8993 - val_loss: 0.0447 - val_score: 0.8690 - val_categorical_accuracy: 0.8579\n",
      "Epoch 8/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0347 - score: 0.9046 - categorical_accuracy: 0.9085 - val_loss: 0.0428 - val_score: 0.8802 - val_categorical_accuracy: 0.8845\n",
      "Epoch 9/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0284 - score: 0.9236 - categorical_accuracy: 0.9222 - val_loss: 0.0392 - val_score: 0.8871 - val_categorical_accuracy: 0.8899\n",
      "Epoch 10/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0243 - score: 0.9383 - categorical_accuracy: 0.9382 - val_loss: 0.0389 - val_score: 0.8936 - val_categorical_accuracy: 0.8899\n",
      "Epoch 11/30\n",
      "1311/1311 [==============================] - 19s 15ms/step - loss: 0.0247 - score: 0.9320 - categorical_accuracy: 0.9336 - val_loss: 0.0373 - val_score: 0.8922 - val_categorical_accuracy: 0.8934\n",
      "Epoch 12/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0207 - score: 0.9507 - categorical_accuracy: 0.9519 - val_loss: 0.0428 - val_score: 0.8809 - val_categorical_accuracy: 0.8721\n",
      "Epoch 13/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0181 - score: 0.9538 - categorical_accuracy: 0.9542 - val_loss: 0.0361 - val_score: 0.9075 - val_categorical_accuracy: 0.9059\n",
      "Epoch 14/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0161 - score: 0.9543 - categorical_accuracy: 0.9565 - val_loss: 0.0352 - val_score: 0.9138 - val_categorical_accuracy: 0.9076\n",
      "Epoch 15/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0146 - score: 0.9649 - categorical_accuracy: 0.9664 - val_loss: 0.0344 - val_score: 0.9087 - val_categorical_accuracy: 0.9094\n",
      "Epoch 16/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0125 - score: 0.9681 - categorical_accuracy: 0.9695 - val_loss: 0.0337 - val_score: 0.9079 - val_categorical_accuracy: 0.9041\n",
      "Epoch 17/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0111 - score: 0.9742 - categorical_accuracy: 0.9725 - val_loss: 0.0324 - val_score: 0.9231 - val_categorical_accuracy: 0.9218\n",
      "Epoch 18/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0099 - score: 0.9750 - categorical_accuracy: 0.9748 - val_loss: 0.0338 - val_score: 0.9245 - val_categorical_accuracy: 0.9147\n",
      "Epoch 19/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0090 - score: 0.9774 - categorical_accuracy: 0.9786 - val_loss: 0.0340 - val_score: 0.9142 - val_categorical_accuracy: 0.9130\n",
      "Epoch 20/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0081 - score: 0.9781 - categorical_accuracy: 0.9779 - val_loss: 0.0324 - val_score: 0.9250 - val_categorical_accuracy: 0.9201\n",
      "Epoch 21/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0071 - score: 0.9792 - categorical_accuracy: 0.9802 - val_loss: 0.0328 - val_score: 0.9207 - val_categorical_accuracy: 0.9147\n",
      "Epoch 22/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0056 - score: 0.9877 - categorical_accuracy: 0.9855 - val_loss: 0.0348 - val_score: 0.9231 - val_categorical_accuracy: 0.9218\n",
      "Epoch 23/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0054 - score: 0.9854 - categorical_accuracy: 0.9855 - val_loss: 0.0335 - val_score: 0.9200 - val_categorical_accuracy: 0.9112\n",
      "Epoch 24/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0048 - score: 0.9896 - categorical_accuracy: 0.9908 - val_loss: 0.0341 - val_score: 0.9209 - val_categorical_accuracy: 0.9183\n",
      "Epoch 25/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0042 - score: 0.9901 - categorical_accuracy: 0.9901 - val_loss: 0.0331 - val_score: 0.9309 - val_categorical_accuracy: 0.9290\n",
      "Epoch 26/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0035 - score: 0.9935 - categorical_accuracy: 0.9931 - val_loss: 0.0343 - val_score: 0.9218 - val_categorical_accuracy: 0.9236\n",
      "Epoch 27/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0035 - score: 0.9916 - categorical_accuracy: 0.9916 - val_loss: 0.0340 - val_score: 0.9143 - val_categorical_accuracy: 0.9183\n",
      "Epoch 28/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0036 - score: 0.9931 - categorical_accuracy: 0.9916 - val_loss: 0.0345 - val_score: 0.9170 - val_categorical_accuracy: 0.9112\n",
      "Epoch 29/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0046 - score: 0.9888 - categorical_accuracy: 0.9893 - val_loss: 0.0362 - val_score: 0.9219 - val_categorical_accuracy: 0.9236\n",
      "Epoch 30/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0034 - score: 0.9939 - categorical_accuracy: 0.9939 - val_loss: 0.0342 - val_score: 0.9317 - val_categorical_accuracy: 0.9307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x180c4ac50>"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(matra_class)),activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
