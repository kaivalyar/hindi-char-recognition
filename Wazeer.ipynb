{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io, morphology, img_as_bool, segmentation\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.ndimage.morphology import binary_fill_holes\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_resize(img):\n",
    "    top = int((224 - img.shape[0])/2)\n",
    "    left = int((224 - img.shape[1])/2)\n",
    "    bottom = 224 - img.shape[0] - top\n",
    "    right = 224 - img.shape[1] - left\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=255)\n",
    "    img = img/255.\n",
    "    img = cv2.resize(img, img_shape) #KADD\n",
    "    return img\n",
    "\n",
    "def skeletonize(img):\n",
    "    size = np.size(img)\n",
    "    skel = np.zeros(img.shape,np.uint8)\n",
    "    img = cv2.bitwise_not(img)\n",
    "#     element = cv2.getStructuringElement(cv2.MORPH_CROSS,(1,1))\n",
    "#     done = 0\n",
    "#     while( done < 1 ):\n",
    "#         eroded = cv2.erode(img,element)\n",
    "#         temp = cv2.dilate(eroded,element)\n",
    "#         temp = cv2.subtract(img,temp)\n",
    "#         skel = cv2.bitwise_or(skel,temp)\n",
    "#         img = eroded.copy()\n",
    "\n",
    "#         zeros = size - cv2.countNonZero(img)\n",
    "#         if zeros==size:#cv2.countNonZero(img) * 1 >= 0:#\n",
    "#             done += 1\n",
    "#     img = skel\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    erosion = cv2.erode(img,kernel,iterations = 2)\n",
    "    img = cv2.bitwise_not(erosion)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Done\n",
      "page0_8_8_2330_2375_2379.png dropped!\n",
      "page0_9_15_2332_2375_2379.png dropped!\n",
      "200 Done\n",
      "300 Done\n",
      "400 Done\n",
      "500 Done\n",
      "600 Done\n",
      "page2_4_20_2346_2366_2390.png dropped!\n",
      "700 Done\n",
      "page3_12_0_2357_2364_2416.png dropped!\n",
      "800 Done\n",
      "page3_18_4_2332_2366_2379.png dropped!\n",
      "900 Done\n",
      "page3_6_0_2327_2417.png dropped!\n",
      "1000 Done\n",
      "1100 Done\n",
      "1200 Done\n",
      "1300 Done\n",
      "page5_7_8_2325_2367_2388.png dropped!\n",
      "1400 Done\n",
      "page6_10_6_2325_2366_2380.png dropped!\n",
      "1500 Done\n",
      "page6_1_1_2340_2375_2379.png dropped!\n",
      "1600 Done\n",
      "1700 Done\n",
      "page7_16_10_2350_2417.png dropped!\n",
      "1800 Done\n",
      "page7_3_0_2319_2366_2367.png dropped!\n",
      "page7_3_13_2319_2366_2367.png dropped!\n"
     ]
    }
   ],
   "source": [
    "PATH = '../train_images_modified'\n",
    "images = []\n",
    "base_class = []\n",
    "matra_class = []\n",
    "dot_class = []\n",
    "total_class = []\n",
    "for filename in os.listdir(PATH):\n",
    "    if filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(PATH,filename),0)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        a,img = cv2.threshold(img,127,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)#KEDIT\n",
    "#         kernel = np.ones((5,5),np.uint8)\n",
    "#         img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#         plt.show()\n",
    "#        img = skeletonize(img)\n",
    "        img = pad_resize(img)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        char_arr = filename[:-4].split('_')[3:]\n",
    "        if(len(char_arr)>0):\n",
    "            char_arr = [int(i) for i in char_arr]\n",
    "            base = [i for i in char_arr if (i>=2308 and i<=2361) or (i==2384) or (i>=2392 and i<=2401) or (i>=2404 and i!=2416 and i!=2417)]  \n",
    "            matra = [i for i in char_arr if i>=2362 and i<=2391 and i!=2364 and i!=2362]\n",
    "            dot = [i for i in char_arr if i==2306 or i==2416 or i==2362 or i==2364]\n",
    "            if(len(matra)>1 or len(dot)>1 or len(base)>1 or (len(matra)==0 and len(dot)==0 and len(char_arr)==2)):\n",
    "                print(filename+\" dropped!\")\n",
    "            else:\n",
    "                images.append(img)\n",
    "                base_class.append(base[0])\n",
    "                if(len(matra)==1):\n",
    "                    matra_class.append(matra[0])\n",
    "                else:\n",
    "                    matra_class.append(0)\n",
    "\n",
    "                if len(dot)==1:\n",
    "                    if dot[0] == 2364:\n",
    "                        dot_class.append(2)\n",
    "                    else:\n",
    "                        dot_class.append(1)\n",
    "                else:\n",
    "                    dot_class.append(0)\n",
    "\n",
    "                if len(images)%100==0:\n",
    "                    print(\"{} Done\".format(len(images)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nliAnvlmCnPhmCfp/HAp7z49tEzsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x185cba4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1883\n",
      "1883\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(images[10],\"gray\")\n",
    "plt.show()\n",
    "print(len(images))\n",
    "print(len(dot_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2392, 2353, 2310, 2384, 2415, 2414]"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(base_class))\n",
    "freq = {i:base_class.count(i) for i in base_class}\n",
    "[i for i in freq.keys() if freq[i]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1883, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "images = np.array(images)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1883, 53)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(base_class)\n",
    "y_labeled = le.transform(base_class)\n",
    "y_train = np_utils.to_categorical(y_labeled)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1883, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.reshape(images,(-1,img_shape[0],img_shape[1],1))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "base_model = Sequential()\n",
    "\n",
    "base_model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "base_model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "base_model.add(MaxPooling2D())\n",
    "base_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "base_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "base_model.add(MaxPooling2D())\n",
    "base_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "base_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "base_model.add(MaxPooling2D())\n",
    "base_model.add(Flatten())\n",
    "base_model.add(Dense(256,activation = 'sigmoid'))\n",
    "base_model.add(Dropout(0.5))\n",
    "base_model.add(Dense(len(set(base_class)),activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_140 (Conv2D)          (None, 62, 62, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_141 (Conv2D)          (None, 60, 60, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_82 (MaxPooling (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_142 (Conv2D)          (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_143 (Conv2D)          (None, 26, 26, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_83 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_144 (Conv2D)          (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_145 (Conv2D)          (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_84 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 53)                13621     \n",
      "=================================================================\n",
      "Total params: 347,813\n",
      "Trainable params: 347,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_score(theta):\n",
    "    def score(y_true, y_pred):\n",
    "\n",
    "        y_thresh = K.cast(K.greater(y_pred,theta),K.floatx())\n",
    "\n",
    "        true_pos =  K.sum(y_true * y_thresh)\n",
    "        false_pos = K.sum(y_true * (1. - y_thresh))\n",
    "        false_neg = K.sum((1. - y_true) * y_thresh)\n",
    "\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        \n",
    "        f1_score_val = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score_val\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1318 samples, validate on 565 samples\n",
      "Epoch 1/40\n",
      "1318/1318 [==============================] - 21s 16ms/step - loss: 3.6166 - score: nan - categorical_accuracy: 0.0781 - val_loss: 3.5257 - val_score: nan - val_categorical_accuracy: 0.0885\n",
      "Epoch 2/40\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 3.3816 - score: nan - categorical_accuracy: 0.0910 - val_loss: 3.5049 - val_score: nan - val_categorical_accuracy: 0.1150\n",
      "Epoch 3/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 3.1320 - score: nan - categorical_accuracy: 0.1942 - val_loss: 3.0036 - val_score: nan - val_categorical_accuracy: 0.2637\n",
      "Epoch 4/40\n",
      "1318/1318 [==============================] - 18s 13ms/step - loss: 2.4822 - score: nan - categorical_accuracy: 0.3619 - val_loss: 2.5489 - val_score: nan - val_categorical_accuracy: 0.4106\n",
      "Epoch 5/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 2.0912 - score: nan - categorical_accuracy: 0.4636 - val_loss: 2.2938 - val_score: 0.3225 - val_categorical_accuracy: 0.4708\n",
      "Epoch 6/40\n",
      "1318/1318 [==============================] - 18s 13ms/step - loss: 1.7505 - score: 0.4600 - categorical_accuracy: 0.5546 - val_loss: 2.0616 - val_score: 0.4530 - val_categorical_accuracy: 0.5204\n",
      "Epoch 7/40\n",
      "1318/1318 [==============================] - 18s 13ms/step - loss: 1.4737 - score: 0.5450 - categorical_accuracy: 0.6305 - val_loss: 1.8348 - val_score: 0.4914 - val_categorical_accuracy: 0.5611\n",
      "Epoch 8/40\n",
      "1318/1318 [==============================] - 19s 15ms/step - loss: 1.2379 - score: 0.6209 - categorical_accuracy: 0.6897 - val_loss: 1.7251 - val_score: 0.5706 - val_categorical_accuracy: 0.6018\n",
      "Epoch 9/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 1.0920 - score: 0.6766 - categorical_accuracy: 0.7223 - val_loss: 1.5210 - val_score: 0.5976 - val_categorical_accuracy: 0.6372\n",
      "Epoch 10/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.9175 - score: 0.7226 - categorical_accuracy: 0.7792 - val_loss: 1.4862 - val_score: 0.6309 - val_categorical_accuracy: 0.6336\n",
      "Epoch 11/40\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 0.7938 - score: 0.7693 - categorical_accuracy: 0.8050 - val_loss: 1.3931 - val_score: 0.6434 - val_categorical_accuracy: 0.6513\n",
      "Epoch 12/40\n",
      "1318/1318 [==============================] - 20s 15ms/step - loss: 0.6719 - score: 0.7983 - categorical_accuracy: 0.8414 - val_loss: 1.3535 - val_score: 0.6630 - val_categorical_accuracy: 0.6726\n",
      "Epoch 13/40\n",
      "1318/1318 [==============================] - 19s 14ms/step - loss: 0.6208 - score: 0.8067 - categorical_accuracy: 0.8323 - val_loss: 1.2627 - val_score: 0.6633 - val_categorical_accuracy: 0.6867\n",
      "Epoch 14/40\n",
      "1318/1318 [==============================] - 19s 14ms/step - loss: 0.5341 - score: 0.8485 - categorical_accuracy: 0.8665 - val_loss: 1.2414 - val_score: 0.6750 - val_categorical_accuracy: 0.6832\n",
      "Epoch 15/40\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 0.4482 - score: 0.8667 - categorical_accuracy: 0.8892 - val_loss: 1.1921 - val_score: 0.6924 - val_categorical_accuracy: 0.6973\n",
      "Epoch 16/40\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 0.3654 - score: 0.9062 - categorical_accuracy: 0.9196 - val_loss: 1.1591 - val_score: 0.7076 - val_categorical_accuracy: 0.7062\n",
      "Epoch 17/40\n",
      "1318/1318 [==============================] - 19s 15ms/step - loss: 0.3559 - score: 0.8978 - categorical_accuracy: 0.9203 - val_loss: 1.1915 - val_score: 0.6940 - val_categorical_accuracy: 0.6938\n",
      "Epoch 18/40\n",
      "1318/1318 [==============================] - 20s 15ms/step - loss: 0.3013 - score: 0.9218 - categorical_accuracy: 0.9416 - val_loss: 1.1328 - val_score: 0.7169 - val_categorical_accuracy: 0.7221\n",
      "Epoch 19/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.2641 - score: 0.9355 - categorical_accuracy: 0.9514 - val_loss: 1.1865 - val_score: 0.7065 - val_categorical_accuracy: 0.6991\n",
      "Epoch 20/40\n",
      "1318/1318 [==============================] - 19s 14ms/step - loss: 0.2664 - score: 0.9316 - categorical_accuracy: 0.9530 - val_loss: 1.1440 - val_score: 0.7152 - val_categorical_accuracy: 0.7239\n",
      "Epoch 21/40\n",
      "1318/1318 [==============================] - 19s 14ms/step - loss: 0.2403 - score: 0.9372 - categorical_accuracy: 0.9469 - val_loss: 1.1580 - val_score: 0.7253 - val_categorical_accuracy: 0.7097\n",
      "Epoch 22/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.2164 - score: 0.9474 - categorical_accuracy: 0.9583 - val_loss: 1.1251 - val_score: 0.7378 - val_categorical_accuracy: 0.7221\n",
      "Epoch 23/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.1770 - score: 0.9619 - categorical_accuracy: 0.9681 - val_loss: 1.1059 - val_score: 0.7379 - val_categorical_accuracy: 0.7221\n",
      "Epoch 24/40\n",
      "1318/1318 [==============================] - 18s 13ms/step - loss: 0.1455 - score: 0.9670 - categorical_accuracy: 0.9727 - val_loss: 1.1135 - val_score: 0.7426 - val_categorical_accuracy: 0.7292\n",
      "Epoch 25/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.1412 - score: 0.9712 - categorical_accuracy: 0.9765 - val_loss: 1.0826 - val_score: 0.7475 - val_categorical_accuracy: 0.7310\n",
      "Epoch 26/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.1192 - score: 0.9790 - categorical_accuracy: 0.9825 - val_loss: 1.0836 - val_score: 0.7535 - val_categorical_accuracy: 0.7469\n",
      "Epoch 27/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.1166 - score: 0.9788 - categorical_accuracy: 0.9841 - val_loss: 1.1194 - val_score: 0.7413 - val_categorical_accuracy: 0.7292\n",
      "Epoch 28/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.1020 - score: 0.9800 - categorical_accuracy: 0.9856 - val_loss: 1.1360 - val_score: 0.7440 - val_categorical_accuracy: 0.7310\n",
      "Epoch 29/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0875 - score: 0.9860 - categorical_accuracy: 0.9917 - val_loss: 1.0867 - val_score: 0.7470 - val_categorical_accuracy: 0.7381\n",
      "Epoch 30/40\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 0.0758 - score: 0.9892 - categorical_accuracy: 0.9924 - val_loss: 1.1318 - val_score: 0.7520 - val_categorical_accuracy: 0.7345\n",
      "Epoch 31/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0798 - score: 0.9876 - categorical_accuracy: 0.9901 - val_loss: 1.1462 - val_score: 0.7489 - val_categorical_accuracy: 0.7292\n",
      "Epoch 32/40\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0708 - score: 0.9912 - categorical_accuracy: 0.9924 - val_loss: 1.1329 - val_score: 0.7502 - val_categorical_accuracy: 0.7398\n",
      "Epoch 33/40\n",
      "1318/1318 [==============================] - 18s 13ms/step - loss: 0.0598 - score: 0.9946 - categorical_accuracy: 0.9954 - val_loss: 1.1076 - val_score: 0.7597 - val_categorical_accuracy: 0.7381\n",
      "Epoch 34/40\n",
      "1318/1318 [==============================] - 19s 14ms/step - loss: 0.0568 - score: 0.9939 - categorical_accuracy: 0.9947 - val_loss: 1.2074 - val_score: 0.7478 - val_categorical_accuracy: 0.7327\n",
      "Epoch 35/40\n",
      "1318/1318 [==============================] - 20s 15ms/step - loss: 0.0578 - score: 0.9931 - categorical_accuracy: 0.9939 - val_loss: 1.1310 - val_score: 0.7581 - val_categorical_accuracy: 0.7451\n",
      "Epoch 36/40\n",
      "  64/1318 [>.............................] - ETA: 14s - loss: 0.0591 - score: 0.9762 - categorical_accuracy: 0.9844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-770-068d57ccac04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_model.fit(x_train,y_train,epochs=40,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOT CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dot = np_utils.to_categorical(dot_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1318 samples, validate on 565 samples\n",
      "Epoch 1/10\n",
      "1318/1318 [==============================] - 22s 17ms/step - loss: 0.2554 - score: 0.8867 - categorical_accuracy: 0.8930 - val_loss: 0.2140 - val_score: 0.9133 - val_categorical_accuracy: 0.9133\n",
      "Epoch 2/10\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.1957 - score: 0.9340 - categorical_accuracy: 0.9340 - val_loss: 0.2118 - val_score: 0.9133 - val_categorical_accuracy: 0.9133\n",
      "Epoch 3/10\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.1860 - score: 0.9340 - categorical_accuracy: 0.9340 - val_loss: 0.2111 - val_score: 0.9133 - val_categorical_accuracy: 0.9133\n",
      "Epoch 4/10\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.1820 - score: 0.9336 - categorical_accuracy: 0.9332 - val_loss: 0.2114 - val_score: 0.9133 - val_categorical_accuracy: 0.9133\n",
      "Epoch 5/10\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.1843 - score: 0.9340 - categorical_accuracy: 0.9340 - val_loss: 0.2140 - val_score: 0.9133 - val_categorical_accuracy: 0.9133\n",
      "Epoch 6/10\n",
      "1318/1318 [==============================] - 18s 13ms/step - loss: 0.1777 - score: 0.9340 - categorical_accuracy: 0.9340 - val_loss: 0.2014 - val_score: 0.9133 - val_categorical_accuracy: 0.9133\n",
      "Epoch 7/10\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.1616 - score: 0.9340 - categorical_accuracy: 0.9340 - val_loss: 0.1755 - val_score: 0.9133 - val_categorical_accuracy: 0.9133\n",
      "Epoch 8/10\n",
      "1318/1318 [==============================] - 21s 16ms/step - loss: 0.1374 - score: 0.9359 - categorical_accuracy: 0.9355 - val_loss: 0.2008 - val_score: 0.9133 - val_categorical_accuracy: 0.9133\n",
      "Epoch 9/10\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 0.1199 - score: 0.9396 - categorical_accuracy: 0.9401 - val_loss: 0.1315 - val_score: 0.9262 - val_categorical_accuracy: 0.9221\n",
      "Epoch 10/10\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0920 - score: 0.9571 - categorical_accuracy: 0.9568 - val_loss: 0.1386 - val_score: 0.9310 - val_categorical_accuracy: 0.9310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x179f9f2e8>"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_model = Sequential()\n",
    "\n",
    "dot_model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "dot_model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "dot_model.add(MaxPooling2D())\n",
    "dot_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "dot_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "dot_model.add(MaxPooling2D())\n",
    "dot_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "dot_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "dot_model.add(MaxPooling2D())\n",
    "dot_model.add(Flatten())\n",
    "dot_model.add(Dense(256,activation = 'sigmoid'))\n",
    "dot_model.add(Dropout(0.5))\n",
    "dot_model.add(Dense(len(set(dot_class)),activation = 'softmax'))\n",
    "\n",
    "dot_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "\n",
    "dot_model.fit(x_train,y_train_dot,epochs=10,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATRA CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1883, 16)\n"
     ]
    }
   ],
   "source": [
    "le_matra = LabelEncoder()\n",
    "le_matra.fit(matra_class)\n",
    "y_labeled_matra = le_matra.transform(matra_class)\n",
    "y_train_matra = np_utils.to_categorical(y_labeled_matra)\n",
    "\n",
    "print(y_train_matra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1318 samples, validate on 565 samples\n",
      "Epoch 1/30\n",
      "1318/1318 [==============================] - 21s 16ms/step - loss: 0.1822 - score: nan - categorical_accuracy: 0.4105 - val_loss: 0.1662 - val_score: nan - val_categorical_accuracy: 0.4372\n",
      "Epoch 2/30\n",
      "1318/1318 [==============================] - 19s 15ms/step - loss: 0.1440 - score: 0.4551 - categorical_accuracy: 0.5273 - val_loss: 0.1155 - val_score: 0.5854 - val_categorical_accuracy: 0.6496\n",
      "Epoch 3/30\n",
      "1318/1318 [==============================] - 21s 16ms/step - loss: 0.0995 - score: 0.6712 - categorical_accuracy: 0.6950 - val_loss: 0.0863 - val_score: 0.7489 - val_categorical_accuracy: 0.7398\n",
      "Epoch 4/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0790 - score: 0.7599 - categorical_accuracy: 0.7648 - val_loss: 0.0718 - val_score: 0.7781 - val_categorical_accuracy: 0.7717\n",
      "Epoch 5/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0633 - score: 0.7995 - categorical_accuracy: 0.8035 - val_loss: 0.0643 - val_score: 0.8023 - val_categorical_accuracy: 0.8035\n",
      "Epoch 6/30\n",
      "1318/1318 [==============================] - 19s 14ms/step - loss: 0.0529 - score: 0.8472 - categorical_accuracy: 0.8505 - val_loss: 0.0586 - val_score: 0.8208 - val_categorical_accuracy: 0.8177\n",
      "Epoch 7/30\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 0.0428 - score: 0.8765 - categorical_accuracy: 0.8839 - val_loss: 0.0576 - val_score: 0.8190 - val_categorical_accuracy: 0.8177\n",
      "Epoch 8/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0417 - score: 0.8896 - categorical_accuracy: 0.8900 - val_loss: 0.0502 - val_score: 0.8600 - val_categorical_accuracy: 0.8566\n",
      "Epoch 9/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0338 - score: 0.9111 - categorical_accuracy: 0.9173 - val_loss: 0.0448 - val_score: 0.8745 - val_categorical_accuracy: 0.8708\n",
      "Epoch 10/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0331 - score: 0.9064 - categorical_accuracy: 0.9074 - val_loss: 0.0442 - val_score: 0.8634 - val_categorical_accuracy: 0.8637\n",
      "Epoch 11/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0268 - score: 0.9331 - categorical_accuracy: 0.9363 - val_loss: 0.0430 - val_score: 0.8685 - val_categorical_accuracy: 0.8690\n",
      "Epoch 12/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0228 - score: 0.9448 - categorical_accuracy: 0.9476 - val_loss: 0.0412 - val_score: 0.8797 - val_categorical_accuracy: 0.8796\n",
      "Epoch 13/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0197 - score: 0.9547 - categorical_accuracy: 0.9530 - val_loss: 0.0392 - val_score: 0.8858 - val_categorical_accuracy: 0.8814\n",
      "Epoch 14/30\n",
      "1318/1318 [==============================] - 18s 13ms/step - loss: 0.0171 - score: 0.9609 - categorical_accuracy: 0.9598 - val_loss: 0.0424 - val_score: 0.8802 - val_categorical_accuracy: 0.8832\n",
      "Epoch 15/30\n",
      "1318/1318 [==============================] - 19s 14ms/step - loss: 0.0177 - score: 0.9595 - categorical_accuracy: 0.9605 - val_loss: 0.0412 - val_score: 0.8808 - val_categorical_accuracy: 0.8814\n",
      "Epoch 16/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0151 - score: 0.9710 - categorical_accuracy: 0.9704 - val_loss: 0.0418 - val_score: 0.8858 - val_categorical_accuracy: 0.8850\n",
      "Epoch 17/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0157 - score: 0.9598 - categorical_accuracy: 0.9575 - val_loss: 0.0399 - val_score: 0.8810 - val_categorical_accuracy: 0.8814\n",
      "Epoch 18/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0135 - score: 0.9705 - categorical_accuracy: 0.9681 - val_loss: 0.0380 - val_score: 0.8980 - val_categorical_accuracy: 0.9009\n",
      "Epoch 19/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0111 - score: 0.9775 - categorical_accuracy: 0.9780 - val_loss: 0.0379 - val_score: 0.8996 - val_categorical_accuracy: 0.8938\n",
      "Epoch 20/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0116 - score: 0.9733 - categorical_accuracy: 0.9734 - val_loss: 0.0394 - val_score: 0.8995 - val_categorical_accuracy: 0.8991\n",
      "Epoch 21/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0101 - score: 0.9766 - categorical_accuracy: 0.9788 - val_loss: 0.0386 - val_score: 0.9076 - val_categorical_accuracy: 0.9044\n",
      "Epoch 22/30\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 0.0097 - score: 0.9793 - categorical_accuracy: 0.9788 - val_loss: 0.0385 - val_score: 0.9002 - val_categorical_accuracy: 0.9027\n",
      "Epoch 23/30\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 0.0077 - score: 0.9843 - categorical_accuracy: 0.9856 - val_loss: 0.0390 - val_score: 0.9046 - val_categorical_accuracy: 0.9027\n",
      "Epoch 24/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0073 - score: 0.9844 - categorical_accuracy: 0.9841 - val_loss: 0.0423 - val_score: 0.8914 - val_categorical_accuracy: 0.8903\n",
      "Epoch 25/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0071 - score: 0.9836 - categorical_accuracy: 0.9803 - val_loss: 0.0405 - val_score: 0.8991 - val_categorical_accuracy: 0.8991\n",
      "Epoch 26/30\n",
      "1318/1318 [==============================] - 17s 13ms/step - loss: 0.0060 - score: 0.9897 - categorical_accuracy: 0.9886 - val_loss: 0.0398 - val_score: 0.8981 - val_categorical_accuracy: 0.9027\n",
      "Epoch 27/30\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 0.0053 - score: 0.9885 - categorical_accuracy: 0.9886 - val_loss: 0.0390 - val_score: 0.9018 - val_categorical_accuracy: 0.8991\n",
      "Epoch 28/30\n",
      "1318/1318 [==============================] - 19s 14ms/step - loss: 0.0049 - score: 0.9885 - categorical_accuracy: 0.9886 - val_loss: 0.0402 - val_score: 0.8971 - val_categorical_accuracy: 0.8973\n",
      "Epoch 29/30\n",
      "1318/1318 [==============================] - 18s 14ms/step - loss: 0.0042 - score: 0.9916 - categorical_accuracy: 0.9901 - val_loss: 0.0394 - val_score: 0.9138 - val_categorical_accuracy: 0.9115\n",
      "Epoch 30/30\n",
      " 992/1318 [=====================>........] - ETA: 3s - loss: 0.0034 - score: 0.9934 - categorical_accuracy: 0.9929"
     ]
    }
   ],
   "source": [
    "matra_model = Sequential()\n",
    "\n",
    "matra_model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "matra_model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "matra_model.add(MaxPooling2D())\n",
    "matra_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "matra_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "matra_model.add(MaxPooling2D())\n",
    "matra_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "matra_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "matra_model.add(MaxPooling2D())\n",
    "matra_model.add(Flatten())\n",
    "matra_model.add(Dense(256,activation = 'sigmoid'))\n",
    "matra_model.add(Dropout(0.5))\n",
    "matra_model.add(Dense(len(set(matra_class)),activation = 'softmax'))\n",
    "\n",
    "matra_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "\n",
    "matra_model.fit(x_train,y_train_matra,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
