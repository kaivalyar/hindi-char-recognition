{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io, morphology, img_as_bool, segmentation\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.ndimage.morphology import binary_fill_holes\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_shape = (64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_resize(img):\n",
    "    top = int((224 - img.shape[0])/2)\n",
    "    left = int((224 - img.shape[1])/2)\n",
    "    bottom = 224 - img.shape[0] - top\n",
    "    right = 224 - img.shape[1] - left\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=255)\n",
    "    img = img/255.\n",
    "    img = cv2.resize(img, img_shape) #KADD\n",
    "    return img\n",
    "\n",
    "def skeletonize(img):\n",
    "    size = np.size(img)\n",
    "    skel = np.zeros(img.shape,np.uint8)\n",
    "    img = cv2.bitwise_not(img)\n",
    "#     element = cv2.getStructuringElement(cv2.MORPH_CROSS,(1,1))\n",
    "#     done = 0\n",
    "#     while( done < 1 ):\n",
    "#         eroded = cv2.erode(img,element)\n",
    "#         temp = cv2.dilate(eroded,element)\n",
    "#         temp = cv2.subtract(img,temp)\n",
    "#         skel = cv2.bitwise_or(skel,temp)\n",
    "#         img = eroded.copy()\n",
    "\n",
    "#         zeros = size - cv2.countNonZero(img)\n",
    "#         if zeros==size:#cv2.countNonZero(img) * 1 >= 0:#\n",
    "#             done += 1\n",
    "#     img = skel\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    erosion = cv2.erode(img,kernel,iterations = 2)\n",
    "    img = cv2.bitwise_not(erosion)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ohno page7_16_10_2350_2417.png\n",
      "page1_9_15_2351_2364_2366.png\n",
      "100 Done\n",
      "ohno page3_6_0_2327_2417.png\n",
      "page5_3_15_2351_2364_2366.png\n",
      "page6_1_1_2340_2375_2379.png\n",
      "200 Done\n",
      "page7_3_0_2319_2366_2367.png\n",
      "page1_16_13_2346_2362_2369.png\n",
      "page6_6_9_2352_2362_2363.png\n",
      "page4_15_2_2357_2364_2376.png\n",
      "page0_15_8_2350_2362_2380.png\n",
      "300 Done\n",
      "page0_8_8_2330_2375_2379.png\n",
      "400 Done\n",
      "page6_10_6_2325_2366_2380.png\n",
      "page7_15_14_2349_2362_2366.png\n",
      "page0_9_15_2332_2375_2379.png\n",
      "500 Done\n",
      "600 Done\n",
      "page1_16_0_2325_2362_2380.png\n",
      "700 Done\n",
      "page1_8_0_2357_2364_2366.png\n",
      "page3_18_4_2332_2366_2379.png\n",
      "800 Done\n",
      "900 Done\n",
      "page7_3_13_2319_2366_2367.png\n",
      "page6_17_13_2351_2364_2366.png\n",
      "1000 Done\n",
      "1100 Done\n",
      "page5_17_5_2325_2362_2376.png\n",
      "1200 Done\n",
      "1300 Done\n",
      "page2_4_1_2357_2364_2366.png\n",
      "1400 Done\n",
      "page5_7_8_2325_2367_2388.png\n",
      "1500 Done\n",
      "page2_4_20_2346_2366_2390.png\n",
      "1600 Done\n",
      "1700 Done\n",
      "1800 Done\n"
     ]
    }
   ],
   "source": [
    "PATH = '../train_images_modified'\n",
    "images = []\n",
    "base_class = []\n",
    "matra_class = []\n",
    "dot_class = []\n",
    "total_class = []\n",
    "for filename in os.listdir(PATH):\n",
    "    if filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(PATH,filename),0)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        a,img = cv2.threshold(img,127,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)#KEDIT\n",
    "#         kernel = np.ones((5,5),np.uint8)\n",
    "#         img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#         plt.show()\n",
    "#        img = skeletonize(img)\n",
    "        img = pad_resize(img)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        char_arr = filename[:-4].split('_')[3:]\n",
    "        if(len(char_arr)>0):\n",
    "            images.append(img)\n",
    "            char_arr = [int(i) for i in char_arr]\n",
    "            total_class.append(char_arr)\n",
    "            base = [i for i in char_arr if (i>=2308 and i<=2361) or (i==2384) or (i>=2392 and i<=2401) or (i>=2404 and i!=2416 and i!=2417)] \n",
    "            assert len(base) != 0, 'too many base classes in the same image (base - {}) (filename - {})'.format(base, filename)\n",
    "            base_class.append(base[0])\n",
    "            matra = [i for i in char_arr if i>=2362 and i<=2391]\n",
    "            dot = [i for i in char_arr if i==2306 or i==2416]\n",
    "            if(len(matra)>1):\n",
    "                print(filename)\n",
    "            elif(len(matra)>0):\n",
    "                matra_class.append(matra[0])\n",
    "            else:\n",
    "                matra_class.append(0)\n",
    "            if len(dot)>0:\n",
    "                dot_class.append(1)\n",
    "            else:\n",
    "                dot_class.append(0)\n",
    "            if(len(matra)==0 and len(dot)==0 and len(char_arr)==2):\n",
    "                print(\"ohno \"+filename)\n",
    "#             if(len(char_arr)>1):          \n",
    "#                 matra_class.append(char_arr[1:])\n",
    "            if len(images)%100==0:\n",
    "                print(\"{} Done\".format(len(images)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsww58c0y5MQ3y9D/AdlzJzYhBVc5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5c73bc9f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(images[10],\"gray\")\n",
    "plt.show()\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2392, 2353, 2384, 2415, 2310, 2414]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(base_class))\n",
    "freq = {i:base_class.count(i) for i in base_class}\n",
    "[i for i in freq.keys() if freq[i]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "drop() got an unexpected keyword argument 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4b89aa0a08d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LABEL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_no_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LABEL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: drop() got an unexpected keyword argument 'columns'"
     ]
    }
   ],
   "source": [
    "images = np.array(images)\n",
    "flat = img_shape[0]*img_shape[1]\n",
    "x_data = np.reshape(np.array(images), (-1, flat))\n",
    "y_data = np.array(base_class)\n",
    "\n",
    "df = pd.DataFrame(x_data, y_data)\n",
    "df['LABEL'] = df.index\n",
    "\n",
    "df_no_label = df.drop(columns='LABEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# #print(df.head())\n",
    "# #print(df_no_label)\n",
    "# #print(df['LABEL'])\n",
    "\n",
    "# X_train,X_val,y_train,y_val = train_test_split(df_no_label, df['LABEL'])\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_val.shape)\n",
    "\n",
    "# clf = RandomForestClassifier(max_features='auto', n_estimators=10, max_depth=20)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# scores = cross_val_score(clf, df_no_label, df['LABEL'], cv=3)\n",
    "# print(scores)\n",
    "# scores_f1 = f1_score(clf.predict(X_train),y_train,average='weighted')\n",
    "# print(scores_f1)\n",
    "# scores_f1 = f1_score(clf.predict(X_val),y_val,average='weighted')\n",
    "# print(scores_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(base_class)\n",
    "y_labeled = le.transform(base_class)\n",
    "y_train = np_utils.to_categorical(y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Dense(256,input_shape=(224*224,),activation = 'sigmoid'))\n",
    "# model.add(Dense(128,activation = 'sigmoid'))\n",
    "# model.add(Dense(128,activation = 'sigmoid'))\n",
    "# model.add(Dense(len(total_char_set),activation = 'sigmoid'))\n",
    "\n",
    "# print(model.summary())\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=[f1_score(theta=0.5),'accuracy'])\n",
    "# X_train = X_train.reshape((-1,224*224))\n",
    "# model.fit(X_train,y_train2,epochs=10,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1895, 53)\n",
      "(1895, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "x_train = np.reshape(images,(-1,img_shape[0],img_shape[1],1))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(base_class)),activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 53)                13621     \n",
      "=================================================================\n",
      "Total params: 347,813\n",
      "Trainable params: 347,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_score(theta):\n",
    "    def score(y_true, y_pred):\n",
    "\n",
    "        y_thresh = K.cast(K.greater(y_pred,theta),K.floatx())\n",
    "\n",
    "        true_pos =  K.sum(y_true * y_thresh)\n",
    "        false_pos = K.sum(y_true * (1. - y_thresh))\n",
    "        false_neg = K.sum((1. - y_true) * y_thresh)\n",
    "\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        \n",
    "        f1_score_val = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score_val\n",
    "    return score\n",
    "\n",
    "def custom_metric(y_true, y_pred):\n",
    "    return K.cast(K.equal(y_true,\n",
    "                          K.round(y_pred)),\n",
    "                  K.floatx())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1326 samples, validate on 569 samples\n",
      "Epoch 1/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 3.5899 - score: nan - categorical_accuracy: 0.0792 - val_loss: 3.3522 - val_score: nan - val_categorical_accuracy: 0.0598\n",
      "Epoch 2/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 3.4525 - score: nan - categorical_accuracy: 0.0754 - val_loss: 3.3754 - val_score: nan - val_categorical_accuracy: 0.1160\n",
      "Epoch 3/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 3.4107 - score: nan - categorical_accuracy: 0.0671 - val_loss: 3.3530 - val_score: nan - val_categorical_accuracy: 0.1195\n",
      "Epoch 4/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 3.3114 - score: nan - categorical_accuracy: 0.1094 - val_loss: 3.2145 - val_score: nan - val_categorical_accuracy: 0.1072\n",
      "Epoch 5/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 2.8439 - score: nan - categorical_accuracy: 0.2760 - val_loss: 2.3376 - val_score: 0.1971 - val_categorical_accuracy: 0.4411\n",
      "Epoch 6/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 2.0331 - score: 0.3448 - categorical_accuracy: 0.5181 - val_loss: 1.7735 - val_score: 0.4830 - val_categorical_accuracy: 0.5852\n",
      "Epoch 7/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 1.6477 - score: 0.4957 - categorical_accuracy: 0.6018 - val_loss: 1.5059 - val_score: 0.5662 - val_categorical_accuracy: 0.6186\n",
      "Epoch 8/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 1.3192 - score: 0.6124 - categorical_accuracy: 0.6825 - val_loss: 1.2824 - val_score: 0.6506 - val_categorical_accuracy: 0.6907\n",
      "Epoch 9/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 1.0727 - score: 0.6858 - categorical_accuracy: 0.7285 - val_loss: 1.1885 - val_score: 0.6747 - val_categorical_accuracy: 0.6872\n",
      "Epoch 10/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.9393 - score: 0.7226 - categorical_accuracy: 0.7549 - val_loss: 1.1614 - val_score: 0.6977 - val_categorical_accuracy: 0.7083\n",
      "Epoch 11/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.7897 - score: 0.7793 - categorical_accuracy: 0.8122 - val_loss: 0.9610 - val_score: 0.7386 - val_categorical_accuracy: 0.7645\n",
      "Epoch 12/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.6482 - score: 0.8042 - categorical_accuracy: 0.8416 - val_loss: 0.8939 - val_score: 0.7552 - val_categorical_accuracy: 0.7645\n",
      "Epoch 13/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.5359 - score: 0.8532 - categorical_accuracy: 0.8801 - val_loss: 0.8533 - val_score: 0.7753 - val_categorical_accuracy: 0.7909\n",
      "Epoch 14/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.4576 - score: 0.8645 - categorical_accuracy: 0.8989 - val_loss: 0.8075 - val_score: 0.7886 - val_categorical_accuracy: 0.7979\n",
      "Epoch 15/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.3947 - score: 0.8865 - categorical_accuracy: 0.9118 - val_loss: 0.7756 - val_score: 0.7909 - val_categorical_accuracy: 0.8014\n",
      "Epoch 16/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.3408 - score: 0.9089 - categorical_accuracy: 0.9336 - val_loss: 0.7550 - val_score: 0.7974 - val_categorical_accuracy: 0.8084\n",
      "Epoch 17/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.2981 - score: 0.9265 - categorical_accuracy: 0.9374 - val_loss: 0.7469 - val_score: 0.8059 - val_categorical_accuracy: 0.8032\n",
      "Epoch 18/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.2565 - score: 0.9345 - categorical_accuracy: 0.9480 - val_loss: 0.7404 - val_score: 0.8036 - val_categorical_accuracy: 0.8137\n",
      "Epoch 19/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.2279 - score: 0.9428 - categorical_accuracy: 0.9563 - val_loss: 0.7307 - val_score: 0.8038 - val_categorical_accuracy: 0.7926\n",
      "Epoch 20/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.2020 - score: 0.9490 - categorical_accuracy: 0.9615 - val_loss: 0.7246 - val_score: 0.8124 - val_categorical_accuracy: 0.7979\n",
      "Epoch 21/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.1834 - score: 0.9574 - categorical_accuracy: 0.9638 - val_loss: 0.6778 - val_score: 0.8250 - val_categorical_accuracy: 0.8102\n",
      "Epoch 22/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.1654 - score: 0.9613 - categorical_accuracy: 0.9706 - val_loss: 0.6764 - val_score: 0.8346 - val_categorical_accuracy: 0.8260\n",
      "Epoch 23/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.1442 - score: 0.9694 - categorical_accuracy: 0.9781 - val_loss: 0.6780 - val_score: 0.8273 - val_categorical_accuracy: 0.8243\n",
      "Epoch 24/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.1362 - score: 0.9684 - categorical_accuracy: 0.9789 - val_loss: 0.6672 - val_score: 0.8323 - val_categorical_accuracy: 0.8330\n",
      "Epoch 25/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.1188 - score: 0.9731 - categorical_accuracy: 0.9811 - val_loss: 0.6674 - val_score: 0.8261 - val_categorical_accuracy: 0.8190\n",
      "Epoch 26/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.1182 - score: 0.9733 - categorical_accuracy: 0.9811 - val_loss: 0.6874 - val_score: 0.8315 - val_categorical_accuracy: 0.8348\n",
      "Epoch 27/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.1024 - score: 0.9791 - categorical_accuracy: 0.9849 - val_loss: 0.6623 - val_score: 0.8198 - val_categorical_accuracy: 0.8207\n",
      "Epoch 28/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.0902 - score: 0.9847 - categorical_accuracy: 0.9872 - val_loss: 0.6661 - val_score: 0.8337 - val_categorical_accuracy: 0.8313\n",
      "Epoch 29/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.0830 - score: 0.9863 - categorical_accuracy: 0.9887 - val_loss: 0.6426 - val_score: 0.8415 - val_categorical_accuracy: 0.8278\n",
      "Epoch 30/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.0857 - score: 0.9854 - categorical_accuracy: 0.9910 - val_loss: 0.6833 - val_score: 0.8339 - val_categorical_accuracy: 0.8207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5c52459400>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOT CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(dot_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1326 samples, validate on 569 samples\n",
      "Epoch 1/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.3037 - score: 0.9186 - categorical_accuracy: 0.9186 - val_loss: 0.2015 - val_score: 0.9490 - val_categorical_accuracy: 0.9490\n",
      "Epoch 2/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.2689 - score: 0.9367 - categorical_accuracy: 0.9367 - val_loss: 0.2016 - val_score: 0.9490 - val_categorical_accuracy: 0.9490\n",
      "Epoch 3/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.2674 - score: 0.9374 - categorical_accuracy: 0.9374 - val_loss: 0.2106 - val_score: 0.9490 - val_categorical_accuracy: 0.9490\n",
      "Epoch 4/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.2637 - score: 0.9374 - categorical_accuracy: 0.9374 - val_loss: 0.2024 - val_score: 0.9490 - val_categorical_accuracy: 0.9490\n",
      "Epoch 5/30\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 0.2424 - score: 0.9367 - categorical_accuracy: 0.9367 - val_loss: 0.1875 - val_score: 0.9490 - val_categorical_accuracy: 0.9490\n",
      "Epoch 6/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.2138 - score: 0.9359 - categorical_accuracy: 0.9359 - val_loss: 0.1556 - val_score: 0.9490 - val_categorical_accuracy: 0.9490\n",
      "Epoch 7/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.1738 - score: 0.9412 - categorical_accuracy: 0.9412 - val_loss: 0.1396 - val_score: 0.9508 - val_categorical_accuracy: 0.9508\n",
      "Epoch 8/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.1558 - score: 0.9427 - categorical_accuracy: 0.9427 - val_loss: 0.1306 - val_score: 0.9578 - val_categorical_accuracy: 0.9578\n",
      "Epoch 9/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.1301 - score: 0.9487 - categorical_accuracy: 0.9487 - val_loss: 0.1264 - val_score: 0.9701 - val_categorical_accuracy: 0.9701\n",
      "Epoch 10/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.1210 - score: 0.9532 - categorical_accuracy: 0.9532 - val_loss: 0.1270 - val_score: 0.9596 - val_categorical_accuracy: 0.9596\n",
      "Epoch 11/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.1001 - score: 0.9623 - categorical_accuracy: 0.9623 - val_loss: 0.1316 - val_score: 0.9578 - val_categorical_accuracy: 0.9578\n",
      "Epoch 12/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.0911 - score: 0.9653 - categorical_accuracy: 0.9653 - val_loss: 0.1665 - val_score: 0.9525 - val_categorical_accuracy: 0.9525\n",
      "Epoch 13/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.0957 - score: 0.9638 - categorical_accuracy: 0.9638 - val_loss: 0.1425 - val_score: 0.9613 - val_categorical_accuracy: 0.9613\n",
      "Epoch 14/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.0709 - score: 0.9744 - categorical_accuracy: 0.9744 - val_loss: 0.1326 - val_score: 0.9596 - val_categorical_accuracy: 0.9596\n",
      "Epoch 15/30\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 0.0596 - score: 0.9827 - categorical_accuracy: 0.9827 - val_loss: 0.1418 - val_score: 0.9578 - val_categorical_accuracy: 0.9578\n",
      "Epoch 16/30\n",
      "1326/1326 [==============================] - 15s 12ms/step - loss: 0.0473 - score: 0.9864 - categorical_accuracy: 0.9864 - val_loss: 0.1663 - val_score: 0.9561 - val_categorical_accuracy: 0.9561\n",
      "Epoch 17/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.0403 - score: 0.9864 - categorical_accuracy: 0.9864 - val_loss: 0.1686 - val_score: 0.9525 - val_categorical_accuracy: 0.9525\n",
      "Epoch 18/30\n",
      "1326/1326 [==============================] - 15s 12ms/step - loss: 0.0244 - score: 0.9962 - categorical_accuracy: 0.9962 - val_loss: 0.1795 - val_score: 0.9596 - val_categorical_accuracy: 0.9596\n",
      "Epoch 19/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.0145 - score: 0.9977 - categorical_accuracy: 0.9977 - val_loss: 0.1992 - val_score: 0.9596 - val_categorical_accuracy: 0.9596\n",
      "Epoch 20/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.0153 - score: 0.9947 - categorical_accuracy: 0.9947 - val_loss: 0.1946 - val_score: 0.9561 - val_categorical_accuracy: 0.9561\n",
      "Epoch 21/30\n",
      "1326/1326 [==============================] - 15s 12ms/step - loss: 0.0159 - score: 0.9962 - categorical_accuracy: 0.9962 - val_loss: 0.2214 - val_score: 0.9613 - val_categorical_accuracy: 0.9613\n",
      "Epoch 22/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.0088 - score: 0.9985 - categorical_accuracy: 0.9985 - val_loss: 0.2242 - val_score: 0.9596 - val_categorical_accuracy: 0.9596\n",
      "Epoch 23/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 0.0058 - score: 0.9985 - categorical_accuracy: 0.9985 - val_loss: 0.2310 - val_score: 0.9561 - val_categorical_accuracy: 0.9561\n",
      "Epoch 24/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.0072 - score: 0.9985 - categorical_accuracy: 0.9985 - val_loss: 0.2182 - val_score: 0.9666 - val_categorical_accuracy: 0.9666\n",
      "Epoch 25/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.0032 - score: 0.9992 - categorical_accuracy: 0.9992 - val_loss: 0.2435 - val_score: 0.9666 - val_categorical_accuracy: 0.9666\n",
      "Epoch 26/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 0.0029 - score: 0.9992 - categorical_accuracy: 0.9992 - val_loss: 0.2435 - val_score: 0.9596 - val_categorical_accuracy: 0.9596\n",
      "Epoch 27/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.0010 - score: 1.0000 - categorical_accuracy: 1.0000 - val_loss: 0.2530 - val_score: 0.9596 - val_categorical_accuracy: 0.9596\n",
      "Epoch 28/30\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 0.0010 - score: 1.0000 - categorical_accuracy: 1.0000 - val_loss: 0.2610 - val_score: 0.9561 - val_categorical_accuracy: 0.9561\n",
      "Epoch 29/30\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 5.7100e-04 - score: 1.0000 - categorical_accuracy: 1.0000 - val_loss: 0.2623 - val_score: 0.9596 - val_categorical_accuracy: 0.9596\n",
      "Epoch 30/30\n",
      "1326/1326 [==============================] - 14s 11ms/step - loss: 3.8470e-04 - score: 1.0000 - categorical_accuracy: 1.0000 - val_loss: 0.2700 - val_score: 0.9561 - val_categorical_accuracy: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5c401e88d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(dot_class)),activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATRA CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page1_9_15_2351_2364_2366.png\n",
      "100 Done\n",
      "page5_3_15_2351_2364_2366.png\n",
      "page6_1_1_2340_2375_2379.png\n",
      "200 Done\n",
      "page7_3_0_2319_2366_2367.png\n",
      "page1_16_13_2346_2362_2369.png\n",
      "page6_6_9_2352_2362_2363.png\n",
      "page4_15_2_2357_2364_2376.png\n",
      "page0_15_8_2350_2362_2380.png\n",
      "300 Done\n",
      "page0_8_8_2330_2375_2379.png\n",
      "400 Done\n",
      "page6_10_6_2325_2366_2380.png\n",
      "page7_15_14_2349_2362_2366.png\n",
      "page0_9_15_2332_2375_2379.png\n",
      "500 Done\n",
      "600 Done\n",
      "page1_16_0_2325_2362_2380.png\n",
      "700 Done\n",
      "page1_8_0_2357_2364_2366.png\n",
      "page3_18_4_2332_2366_2379.png\n",
      "800 Done\n",
      "900 Done\n",
      "page7_3_13_2319_2366_2367.png\n",
      "page6_17_13_2351_2364_2366.png\n",
      "1000 Done\n",
      "1100 Done\n",
      "page5_17_5_2325_2362_2376.png\n",
      "1200 Done\n",
      "1300 Done\n",
      "page2_4_1_2357_2364_2366.png\n",
      "1400 Done\n",
      "page5_7_8_2325_2367_2388.png\n",
      "1500 Done\n",
      "page2_4_20_2346_2366_2390.png\n",
      "1600 Done\n",
      "1700 Done\n",
      "1800 Done\n"
     ]
    }
   ],
   "source": [
    "PATH = '../train_images_modified'\n",
    "images = []\n",
    "matra_class = []\n",
    "for filename in os.listdir(PATH):\n",
    "    if filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(PATH,filename),0)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        a,img = cv2.threshold(img,127,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)#KEDIT\n",
    "#         kernel = np.ones((5,5),np.uint8)\n",
    "#         img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#         plt.show()\n",
    "#        img = skeletonize(img)\n",
    "        img = pad_resize(img)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        char_arr = filename[:-4].split('_')[3:]\n",
    "        if(len(char_arr)>0):\n",
    "            char_arr = [int(i) for i in char_arr]\n",
    "            matra = [i for i in char_arr if i>=2362 and i<=2391]\n",
    "            if(len(matra)>1):\n",
    "                print(filename)\n",
    "            elif(len(matra)>0):\n",
    "                images.append(img)\n",
    "                matra_class.append(matra[0])\n",
    "            else:\n",
    "                images.append(img)\n",
    "                matra_class.append('0')\n",
    "            if len(images)%100==0:\n",
    "                print(\"{} Done\".format(len(images)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1874, 17)\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(matra_class)\n",
    "y_labeled = le.transform(matra_class)\n",
    "y_train = np_utils.to_categorical(y_labeled)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_labeled)\n",
    "images = np.array(images)\n",
    "x_train = np.reshape(images,(-1,img_shape[0],img_shape[1],1))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1311 samples, validate on 563 samples\n",
      "Epoch 1/30\n",
      "1311/1311 [==============================] - 12s 9ms/step - loss: 0.1711 - score: nan - categorical_accuracy: 0.4249 - val_loss: 0.1559 - val_score: 0.4654 - val_categorical_accuracy: 0.4654\n",
      "Epoch 2/30\n",
      "1311/1311 [==============================] - 15s 11ms/step - loss: 0.1375 - score: 0.4606 - categorical_accuracy: 0.5500 - val_loss: 0.1032 - val_score: 0.6587 - val_categorical_accuracy: 0.6661\n",
      "Epoch 3/30\n",
      "1311/1311 [==============================] - 13s 10ms/step - loss: 0.0919 - score: 0.6984 - categorical_accuracy: 0.7109 - val_loss: 0.0843 - val_score: 0.7326 - val_categorical_accuracy: 0.7247\n",
      "Epoch 4/30\n",
      "1311/1311 [==============================] - 14s 11ms/step - loss: 0.0760 - score: 0.7458 - categorical_accuracy: 0.7567 - val_loss: 0.0723 - val_score: 0.7500 - val_categorical_accuracy: 0.7531\n",
      "Epoch 5/30\n",
      "1311/1311 [==============================] - 14s 11ms/step - loss: 0.0670 - score: 0.7779 - categorical_accuracy: 0.7818 - val_loss: 0.0572 - val_score: 0.8128 - val_categorical_accuracy: 0.8384\n",
      "Epoch 6/30\n",
      "1311/1311 [==============================] - 14s 10ms/step - loss: 0.0535 - score: 0.8272 - categorical_accuracy: 0.8490 - val_loss: 0.0525 - val_score: 0.8434 - val_categorical_accuracy: 0.8526\n",
      "Epoch 7/30\n",
      "1311/1311 [==============================] - 16s 12ms/step - loss: 0.0438 - score: 0.8701 - categorical_accuracy: 0.8726 - val_loss: 0.0456 - val_score: 0.8639 - val_categorical_accuracy: 0.8668\n",
      "Epoch 8/30\n",
      "1311/1311 [==============================] - 15s 11ms/step - loss: 0.0400 - score: 0.8822 - categorical_accuracy: 0.8856 - val_loss: 0.0411 - val_score: 0.8840 - val_categorical_accuracy: 0.8845\n",
      "Epoch 9/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0338 - score: 0.9109 - categorical_accuracy: 0.9153 - val_loss: 0.0437 - val_score: 0.8643 - val_categorical_accuracy: 0.8597\n",
      "Epoch 10/30\n",
      "1311/1311 [==============================] - 24s 19ms/step - loss: 0.0283 - score: 0.9208 - categorical_accuracy: 0.9191 - val_loss: 0.0382 - val_score: 0.8947 - val_categorical_accuracy: 0.8881\n",
      "Epoch 11/30\n",
      "1311/1311 [==============================] - 23s 17ms/step - loss: 0.0241 - score: 0.9351 - categorical_accuracy: 0.9367 - val_loss: 0.0367 - val_score: 0.8880 - val_categorical_accuracy: 0.8863\n",
      "Epoch 12/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0214 - score: 0.9415 - categorical_accuracy: 0.9436 - val_loss: 0.0347 - val_score: 0.9034 - val_categorical_accuracy: 0.9130\n",
      "Epoch 13/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0188 - score: 0.9501 - categorical_accuracy: 0.9527 - val_loss: 0.0366 - val_score: 0.9009 - val_categorical_accuracy: 0.8952\n",
      "Epoch 14/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0197 - score: 0.9451 - categorical_accuracy: 0.9428 - val_loss: 0.0375 - val_score: 0.8957 - val_categorical_accuracy: 0.8899\n",
      "Epoch 15/30\n",
      "1311/1311 [==============================] - 18s 14ms/step - loss: 0.0146 - score: 0.9604 - categorical_accuracy: 0.9603 - val_loss: 0.0346 - val_score: 0.9084 - val_categorical_accuracy: 0.9147\n",
      "Epoch 16/30\n",
      "1311/1311 [==============================] - 19s 14ms/step - loss: 0.0139 - score: 0.9591 - categorical_accuracy: 0.9603 - val_loss: 0.0342 - val_score: 0.9121 - val_categorical_accuracy: 0.9094\n",
      "Epoch 17/30\n",
      "1311/1311 [==============================] - 19s 14ms/step - loss: 0.0118 - score: 0.9683 - categorical_accuracy: 0.9680 - val_loss: 0.0358 - val_score: 0.9118 - val_categorical_accuracy: 0.9112\n",
      "Epoch 18/30\n",
      "1311/1311 [==============================] - 19s 15ms/step - loss: 0.0094 - score: 0.9766 - categorical_accuracy: 0.9764 - val_loss: 0.0353 - val_score: 0.9139 - val_categorical_accuracy: 0.9130\n",
      "Epoch 19/30\n",
      "1311/1311 [==============================] - 20s 15ms/step - loss: 0.0095 - score: 0.9762 - categorical_accuracy: 0.9771 - val_loss: 0.0357 - val_score: 0.9136 - val_categorical_accuracy: 0.9130\n",
      "Epoch 20/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0108 - score: 0.9698 - categorical_accuracy: 0.9695 - val_loss: 0.0360 - val_score: 0.9183 - val_categorical_accuracy: 0.9130\n",
      "Epoch 21/30\n",
      "1311/1311 [==============================] - 14s 11ms/step - loss: 0.0086 - score: 0.9806 - categorical_accuracy: 0.9809 - val_loss: 0.0338 - val_score: 0.9191 - val_categorical_accuracy: 0.9147\n",
      "Epoch 22/30\n",
      "1311/1311 [==============================] - 15s 12ms/step - loss: 0.0063 - score: 0.9862 - categorical_accuracy: 0.9855 - val_loss: 0.0346 - val_score: 0.9215 - val_categorical_accuracy: 0.9183\n",
      "Epoch 23/30\n",
      "1311/1311 [==============================] - 16s 12ms/step - loss: 0.0057 - score: 0.9884 - categorical_accuracy: 0.9886 - val_loss: 0.0384 - val_score: 0.9084 - val_categorical_accuracy: 0.9076\n",
      "Epoch 24/30\n",
      "1311/1311 [==============================] - 14s 11ms/step - loss: 0.0052 - score: 0.9900 - categorical_accuracy: 0.9901 - val_loss: 0.0351 - val_score: 0.9187 - val_categorical_accuracy: 0.9165\n",
      "Epoch 25/30\n",
      "1311/1311 [==============================] - 15s 11ms/step - loss: 0.0048 - score: 0.9893 - categorical_accuracy: 0.9886 - val_loss: 0.0349 - val_score: 0.9229 - val_categorical_accuracy: 0.9272\n",
      "Epoch 26/30\n",
      "1311/1311 [==============================] - 14s 11ms/step - loss: 0.0042 - score: 0.9919 - categorical_accuracy: 0.9916 - val_loss: 0.0365 - val_score: 0.9179 - val_categorical_accuracy: 0.9147\n",
      "Epoch 27/30\n",
      "1311/1311 [==============================] - 14s 11ms/step - loss: 0.0036 - score: 0.9923 - categorical_accuracy: 0.9931 - val_loss: 0.0368 - val_score: 0.9232 - val_categorical_accuracy: 0.9218\n",
      "Epoch 28/30\n",
      "1311/1311 [==============================] - 15s 12ms/step - loss: 0.0040 - score: 0.9927 - categorical_accuracy: 0.9939 - val_loss: 0.0362 - val_score: 0.9222 - val_categorical_accuracy: 0.9254\n",
      "Epoch 29/30\n",
      "1311/1311 [==============================] - 16s 12ms/step - loss: 0.0032 - score: 0.9935 - categorical_accuracy: 0.9924 - val_loss: 0.0384 - val_score: 0.9156 - val_categorical_accuracy: 0.9112\n",
      "Epoch 30/30\n",
      "1311/1311 [==============================] - 17s 13ms/step - loss: 0.0031 - score: 0.9958 - categorical_accuracy: 0.9962 - val_loss: 0.0369 - val_score: 0.9204 - val_categorical_accuracy: 0.9165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5c380ab908>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(matra_class)),activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
