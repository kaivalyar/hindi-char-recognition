{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io, morphology, img_as_bool, segmentation\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.ndimage.morphology import binary_fill_holes\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_shape = (64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_resize(img):\n",
    "    top = int((224 - img.shape[0])/2)\n",
    "    left = int((224 - img.shape[1])/2)\n",
    "    bottom = 224 - img.shape[0] - top\n",
    "    right = 224 - img.shape[1] - left\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=255)\n",
    "    img = img/255.\n",
    "    img = cv2.resize(img, img_shape) #KADD\n",
    "    return img\n",
    "\n",
    "def skeletonize(img):\n",
    "    size = np.size(img)\n",
    "    skel = np.zeros(img.shape,np.uint8)\n",
    "    img = cv2.bitwise_not(img)\n",
    "#     element = cv2.getStructuringElement(cv2.MORPH_CROSS,(1,1))\n",
    "#     done = 0\n",
    "#     while( done < 1 ):\n",
    "#         eroded = cv2.erode(img,element)\n",
    "#         temp = cv2.dilate(eroded,element)\n",
    "#         temp = cv2.subtract(img,temp)\n",
    "#         skel = cv2.bitwise_or(skel,temp)\n",
    "#         img = eroded.copy()\n",
    "\n",
    "#         zeros = size - cv2.countNonZero(img)\n",
    "#         if zeros==size:#cv2.countNonZero(img) * 1 >= 0:#\n",
    "#             done += 1\n",
    "#     img = skel\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    erosion = cv2.erode(img,kernel,iterations = 2)\n",
    "    img = cv2.bitwise_not(erosion)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ohno page7_16_10_2350_2417.png\n",
      "page1_9_15_2351_2364_2366.png\n",
      "100 Done\n",
      "ohno page3_6_0_2327_2417.png\n",
      "page5_3_15_2351_2364_2366.png\n",
      "page6_1_1_2340_2375_2379.png\n",
      "200 Done\n",
      "page7_3_0_2319_2366_2367.png\n",
      "page1_16_13_2346_2362_2369.png\n",
      "page6_6_9_2352_2362_2363.png\n",
      "page4_15_2_2357_2364_2376.png\n",
      "page0_15_8_2350_2362_2380.png\n",
      "300 Done\n",
      "page0_8_8_2330_2375_2379.png\n",
      "400 Done\n",
      "page6_10_6_2325_2366_2380.png\n",
      "page7_15_14_2349_2362_2366.png\n",
      "page0_9_15_2332_2375_2379.png\n",
      "500 Done\n",
      "600 Done\n",
      "page1_16_0_2325_2362_2380.png\n",
      "700 Done\n",
      "page1_8_0_2357_2364_2366.png\n",
      "page3_18_4_2332_2366_2379.png\n",
      "800 Done\n",
      "900 Done\n",
      "page7_3_13_2319_2366_2367.png\n",
      "page6_17_13_2351_2364_2366.png\n",
      "1000 Done\n",
      "1100 Done\n",
      "page5_17_5_2325_2362_2376.png\n",
      "1200 Done\n",
      "1300 Done\n",
      "page2_4_1_2357_2364_2366.png\n",
      "1400 Done\n",
      "page5_7_8_2325_2367_2388.png\n",
      "1500 Done\n",
      "page2_4_20_2346_2366_2390.png\n",
      "1600 Done\n",
      "1700 Done\n",
      "1800 Done\n"
     ]
    }
   ],
   "source": [
    "PATH = '../train_images_modified'\n",
    "images = []\n",
    "base_class = []\n",
    "matra_class = []\n",
    "images_m = []\n",
    "dot_class = []\n",
    "total_class = []\n",
    "for filename in os.listdir(PATH):\n",
    "    if filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(PATH,filename),0)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        a,img = cv2.threshold(img,127,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)#KEDIT\n",
    "#         kernel = np.ones((5,5),np.uint8)\n",
    "#         img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#         plt.show()\n",
    "#        img = skeletonize(img)\n",
    "        img = pad_resize(img)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        char_arr = filename[:-4].split('_')[3:]\n",
    "        if(len(char_arr)>0):\n",
    "            images.append(img)\n",
    "            char_arr = [int(i) for i in char_arr]\n",
    "            total_class.append(char_arr)\n",
    "            base = [i for i in char_arr if (i>=2308 and i<=2361) or (i==2384) or (i>=2392 and i<=2401) or (i>=2404 and i!=2416 and i!=2417)] \n",
    "            assert len(base) != 0, 'too many base classes in the same image (base - {}) (filename - {})'.format(base, filename)\n",
    "            base_class.append(base[0])\n",
    "            matra = [i for i in char_arr if i>=2362 and i<=2391]\n",
    "            dot = [i for i in char_arr if i==2306 or i==2416]\n",
    "            if(len(matra)>1):\n",
    "                print(filename)\n",
    "            elif(len(matra)>0):\n",
    "                matra_class.append(matra[0])\n",
    "                images_m.append(img)\n",
    "            else:\n",
    "                matra_class.append(0)\n",
    "                images_m.append(img)\n",
    "            if len(dot)>0:\n",
    "                dot_class.append(1)\n",
    "            else:\n",
    "                dot_class.append(0)\n",
    "            if(len(matra)==0 and len(dot)==0 and len(char_arr)==2):\n",
    "                print(\"ohno \"+filename)\n",
    "#             if(len(char_arr)>1):          \n",
    "#                 matra_class.append(char_arr[1:])\n",
    "            if len(images)%100==0:\n",
    "                print(\"{} Done\".format(len(images)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsww58c0y5MQ3y9D/AdlzJzYhBVc5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71e08e4e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(images[10],\"gray\")\n",
    "plt.show()\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2392, 2353, 2384, 2415, 2310, 2414]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(base_class))\n",
    "freq = {i:base_class.count(i) for i in base_class}\n",
    "[i for i in freq.keys() if freq[i]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "flat = img_shape[0]*img_shape[1]\n",
    "x_data = np.reshape(np.array(images), (-1, flat))\n",
    "y_data = np.array(base_class)\n",
    "\n",
    "df = pd.DataFrame(x_data, y_data)\n",
    "df['LABEL'] = df.index\n",
    "\n",
    "#df_no_label = df.drop(columns='LABEL')\n",
    "df_no_label = pd.DataFrame(x_data, index=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# #print(df.head())\n",
    "# #print(df_no_label)\n",
    "# #print(df['LABEL'])\n",
    "\n",
    "# X_train,X_val,y_train,y_val = train_test_split(df_no_label, df['LABEL'])\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_val.shape)\n",
    "\n",
    "# clf = RandomForestClassifier(max_features='auto', n_estimators=10, max_depth=20)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# scores = cross_val_score(clf, df_no_label, df['LABEL'], cv=3)\n",
    "# print(scores)\n",
    "# scores_f1 = f1_score(clf.predict(X_train),y_train,average='weighted')\n",
    "# print(scores_f1)\n",
    "# scores_f1 = f1_score(clf.predict(X_val),y_val,average='weighted')\n",
    "# print(scores_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(base_class)\n",
    "y_labeled = le.transform(base_class)\n",
    "y_train = np_utils.to_categorical(y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Dense(256,input_shape=(224*224,),activation = 'sigmoid'))\n",
    "# model.add(Dense(128,activation = 'sigmoid'))\n",
    "# model.add(Dense(128,activation = 'sigmoid'))\n",
    "# model.add(Dense(len(total_char_set),activation = 'sigmoid'))\n",
    "\n",
    "# print(model.summary())\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=[f1_score(theta=0.5),'accuracy'])\n",
    "# X_train = X_train.reshape((-1,224*224))\n",
    "# model.fit(X_train,y_train2,epochs=10,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1895, 53)\n",
      "(1895, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "x_train = np.reshape(images,(-1,img_shape[0],img_shape[1],1))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten\n",
    "\n",
    "base_model = Sequential()\n",
    "\n",
    "base_model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "base_model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "base_model.add(MaxPooling2D())\n",
    "base_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "base_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "base_model.add(MaxPooling2D())\n",
    "base_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "base_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "base_model.add(MaxPooling2D())\n",
    "base_model.add(Flatten())\n",
    "base_model.add(Dense(256,activation = 'sigmoid'))\n",
    "base_model.add(Dropout(0.5))\n",
    "base_model.add(Dense(len(set(base_class)),activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 53)                13621     \n",
      "=================================================================\n",
      "Total params: 347,813\n",
      "Trainable params: 347,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_score(theta):\n",
    "    def score(y_true, y_pred):\n",
    "\n",
    "        y_thresh = K.cast(K.greater(y_pred,theta),K.floatx())\n",
    "\n",
    "        true_pos =  K.sum(y_true * y_thresh)\n",
    "        false_pos = K.sum(y_true * (1. - y_thresh))\n",
    "        false_neg = K.sum((1. - y_true) * y_thresh)\n",
    "\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        \n",
    "        f1_score_val = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score_val\n",
    "    return score\n",
    "\n",
    "def custom_metric(y_true, y_pred):\n",
    "    return K.cast(K.equal(y_true,\n",
    "                          K.round(y_pred)),\n",
    "                  K.floatx())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1326 samples, validate on 569 samples\n",
      "Epoch 1/20\n",
      "1326/1326 [==============================] - 14s 10ms/step - loss: 3.6759 - score: nan - categorical_accuracy: 0.0603 - val_loss: 3.3773 - val_score: nan - val_categorical_accuracy: 0.1160\n",
      "Epoch 2/20\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 3.4225 - score: nan - categorical_accuracy: 0.0890 - val_loss: 3.3516 - val_score: nan - val_categorical_accuracy: 0.0598\n",
      "Epoch 3/20\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 3.3018 - score: nan - categorical_accuracy: 0.1297 - val_loss: 2.9534 - val_score: nan - val_categorical_accuracy: 0.2988\n",
      "Epoch 4/20\n",
      "1326/1326 [==============================] - 13s 10ms/step - loss: 2.5674 - score: nan - categorical_accuracy: 0.3552 - val_loss: 2.0409 - val_score: 0.3142 - val_categorical_accuracy: 0.5149\n",
      "Epoch 5/20\n",
      "1326/1326 [==============================] - 15s 11ms/step - loss: 1.8715 - score: 0.3809 - categorical_accuracy: 0.5407 - val_loss: 1.6249 - val_score: 0.5085 - val_categorical_accuracy: 0.5940\n",
      "Epoch 6/20\n",
      "1326/1326 [==============================] - 17s 13ms/step - loss: 1.4308 - score: 0.5685 - categorical_accuracy: 0.6569 - val_loss: 1.3565 - val_score: 0.6333 - val_categorical_accuracy: 0.6749\n",
      "Epoch 7/20\n",
      " 704/1326 [==============>...............] - ETA: 6s - loss: 1.2351 - score: 0.6263 - categorical_accuracy: 0.6918"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ad2cfab7e048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_model.fit(x_train,y_train,epochs=20,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "base_model.compile(optimizer=new_adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "base_model.fit(x_train,y_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = base_model.predict_classes(x_train)\n",
    "y_original = np.argmax(y_train, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_original, y_pred)\n",
    "print('confusion matrix for {} features (base):'.format(cm.shape))\n",
    "print(cm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(cm, cmap=sns.light_palette('purple', n_colors=500, as_cmap=True), annot=True, linewidths = 5, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOT CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(dot_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_model = Sequential()\n",
    "\n",
    "dot_model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "dot_model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "dot_model.add(MaxPooling2D())\n",
    "dot_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "dot_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "dot_model.add(MaxPooling2D())\n",
    "dot_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "dot_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "dot_model.add(MaxPooling2D())\n",
    "dot_model.add(Flatten())\n",
    "dot_model.add(Dense(256,activation = 'sigmoid'))\n",
    "dot_model.add(Dropout(0.5))\n",
    "dot_model.add(Dense(len(set(dot_class)),activation = 'softmax'))\n",
    "\n",
    "dot_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "\n",
    "dot_model.fit(x_train,y_train,epochs=20,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "dot_model.compile(optimizer=new_adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "dot_model.fit(x_train,y_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = dot_model.predict_classes(x_train)\n",
    "y_original = np.argmax(y_train, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_original, y_pred)\n",
    "print('confusion matrix for {} features (dot):'.format(cm.shape))\n",
    "print(cm)\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(cm, cmap=sns.light_palette('purple', n_colors=500, as_cmap=True), annot=True, linewidths = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATRA CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PATH = '../train_images_modified'\n",
    "images = []\n",
    "matra_class = []\n",
    "for filename in os.listdir(PATH):\n",
    "    if filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(PATH,filename),0)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        a,img = cv2.threshold(img,127,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)#KEDIT\n",
    "#         kernel = np.ones((5,5),np.uint8)\n",
    "#         img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#         plt.show()\n",
    "#        img = skeletonize(img)\n",
    "        img = pad_resize(img)\n",
    "        blur = cv2.GaussianBlur(img,(9,9),0)# KADD\n",
    "        char_arr = filename[:-4].split('_')[3:]\n",
    "        if(len(char_arr)>0):\n",
    "            char_arr = [int(i) for i in char_arr]\n",
    "            matra = [i for i in char_arr if i>=2362 and i<=2391]\n",
    "            if(len(matra)>1):\n",
    "                print(filename)\n",
    "            elif(len(matra)>0):\n",
    "                images.append(img)\n",
    "                matra_class.append(matra[0])\n",
    "            else:\n",
    "                images.append(img)\n",
    "                matra_class.append('0')\n",
    "            if len(images)%100==0:\n",
    "                print(\"{} Done\".format(len(images)))\n",
    "            \n",
    "for i in range(len(images_m)):\n",
    "    print(images_m[i])\n",
    "    print(images[i])\n",
    "    print(images_m[i] == images[i])\n",
    "'''\n",
    "\n",
    "images = images_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(matra_class)\n",
    "y_labeled = le.transform(matra_class)\n",
    "y_train = np_utils.to_categorical(y_labeled)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_labeled)\n",
    "images = np.array(images)\n",
    "x_train = np.reshape(images,(-1,img_shape[0],img_shape[1],1))\n",
    "print(y_train.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matra_model = Sequential()\n",
    "\n",
    "matra_model.add(Conv2D(16,(3,3),input_shape = (img_shape[0],img_shape[1],1),activation = 'relu'))\n",
    "matra_model.add(Conv2D(16,(3,3),activation = 'relu'))\n",
    "matra_model.add(MaxPooling2D())\n",
    "matra_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "matra_model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "matra_model.add(MaxPooling2D())\n",
    "matra_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "matra_model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "matra_model.add(MaxPooling2D())\n",
    "matra_model.add(Flatten())\n",
    "matra_model.add(Dense(256,activation = 'sigmoid'))\n",
    "matra_model.add(Dropout(0.5))\n",
    "matra_model.add(Dense(len(set(matra_class)),activation = 'softmax'))\n",
    "\n",
    "matra_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "\n",
    "matra_model.fit(x_train,y_train,epochs=20,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "matra_model.compile(optimizer=new_adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_score(theta=0.5), 'categorical_accuracy'])\n",
    "matra_model.fit(x_train,y_train,epochs=30,batch_size=32,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = matra_model.predict_classes(x_train)\n",
    "y_original = np.argmax(y_train, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_original, y_pred)\n",
    "print('confusion matrix for {} features (matra):'.format(cm.shape))\n",
    "print(cm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(cm, cmap=sns.light_palette('purple', n_colors=500, as_cmap=True), annot=True, linewidths = 5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
